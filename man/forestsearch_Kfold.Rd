% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/forestsearch_cross-validation.R
\name{forestsearch_Kfold}
\alias{forestsearch_Kfold}
\title{ForestSearch K-Fold Cross-Validation}
\usage{
forestsearch_Kfold(
  fs.est,
  Kfolds = nrow(fs.est$df.est),
  seedit = 8316951,
  parallel_args = list(plan = "multisession", workers = 6, show_message = TRUE),
  sg0.name = "Not recommend",
  sg1.name = "Recommend",
  details = FALSE
)
}
\arguments{
\item{fs.est}{List. ForestSearch results object from \code{\link{forestsearch}}.
Must contain \code{df.est} (data frame) and \code{args_call_all} (list of arguments).}

\item{Kfolds}{Integer. Number of folds (default: \code{nrow(fs.est$df.est)} for LOO).}

\item{seedit}{Integer. Random seed for fold assignment (default: 8316951).}

\item{parallel_args}{List. Parallelization configuration with elements:
\itemize{
\item \code{plan}: Character. One of "multisession", "multicore", "sequential"
\item \code{workers}: Integer. Number of parallel workers
\item \code{show_message}: Logical. Show parallel setup messages
}}

\item{sg0.name}{Character. Label for subgroup 0 (default: "Not recommend").}

\item{sg1.name}{Character. Label for subgroup 1 (default: "Recommend").}

\item{details}{Logical. Print progress details (default: FALSE).}
}
\value{
List with components:
\describe{
\item{resCV}{Data frame with CV predictions for each observation}
\item{cv_args}{Arguments used for CV ForestSearch calls}
\item{timing_minutes}{Execution time in minutes}
\item{prop_SG_found}{Percentage of folds where a subgroup was found}
\item{sg_analysis}{Original subgroup definition from full-data analysis}
\item{sg0.name, sg1.name}{Subgroup labels}
\item{Kfolds}{Number of folds used}
\item{sens_summary}{Named vector of sensitivity metrics (sens_H, sens_Hc, ppv_H, ppv_Hc)}
\item{find_summary}{Named vector of subgroup-finding metrics (Any, Exact, etc.)}
}
}
\description{
This function assesses the stability and reproducibility of ForestSearch
subgroup identification through cross-validation. For each fold:
\enumerate{
\item Train ForestSearch on (K-1) folds
\item Apply the identified subgroup to the held-out fold
\item Compare predictions to the original full-data analysis
}
}
\details{
Performs K-fold cross-validation for ForestSearch, evaluating subgroup
identification and agreement between training and test sets.
}
\section{Cross-Validation Types}{

\itemize{
\item \strong{Leave-One-Out (LOO)}: When \code{Kfolds = nrow(df)}, each
observation is held out once. Most thorough but computationally intensive.
\item \strong{K-Fold}: When \code{Kfolds < nrow(df)}, data is split into K
roughly equal folds. Good balance of bias-variance tradeoff.
}
}

\section{Output Metrics}{

The returned \code{resCV} data frame contains:
\itemize{
\item \code{treat.recommend}: Prediction from CV model
\item \code{treat.recommend.original}: Prediction from full-data model
\item \code{cvindex}: Fold assignment
\item \code{sg1}, \code{sg2}: Subgroup definitions found in each fold
}
}

\examples{
\dontrun{
# Run initial ForestSearch
fs_result <- forestsearch(
  df.analysis = trial_data,
  outcome.name = "time",
  event.name = "status",
  treat.name = "treatment",
  confounders.name = c("age", "biomarker")
)

# Run 10-fold cross-validation
cv_results <- forestsearch_Kfold(
  fs.est = fs_result,
  Kfolds = 10,
  parallel_args = list(plan = "multisession", workers = 4),
  details = TRUE
)

# Summarize results
cv_summary <- forestsearch_KfoldOut(cv_results, outall = TRUE)
}

}
\seealso{
\code{\link{forestsearch}} for initial subgroup identification
\code{\link{forestsearch_KfoldOut}} for summarizing CV results
\code{\link{forestsearch_tenfold}} for repeated K-fold simulations
}
