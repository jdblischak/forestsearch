\Sexpr{set_parent("forestSearch_SIM-Rev1_v0.Rnw")}

<<echo=FALSE>>=
opts_chunk$set (warning = FALSE, message = FALSE, tidy=TRUE, echo=FALSE, dev = 'pdf')
options(warn=-1)
@

\section{Subgroup Identification Approach} \label{sec:SIA}

We consider the two-sample random censorship model with $N$ observations from a randomized clinical trial.
Let $T$ denote the survival time, $C$ the censoring time, $V$ the treatment assignment, 
and $Z=(Z_{1},Z_{2},\ldots,Z_{p})$ a $p$-dimensional collection of baseline covariates.  It is of interest to evaluate subgroups 
formed by combinations of these baseline covariates. We observe the possibly
censored survival time $Y=\min(T,C)$ with $\Delta=I(T \leq C)$ the event indicator. 
The survival times $T$ and censoring times $C$ are assumed to be independent, conditional on $(V,Z)$, with continuous distributions. The observations $(V_{i},Z_{i},Y_{i},\Delta_{i})$ for $i=1,\ldots,N$ are assumed to be iid replicates.

In oncology trials the gold-standard primary ITT analysis is a Cox model with only the treatment arm as a covariate, usually stratified.\cite{ACR_2015,FK_2019}  Standard forest plots often proceed by fitting 

\begin{equation}
\lambda(t;V) = \lambda_{0}(t) \exp(\beta V),
\label{coxsg}
\end{equation}

\noindent within the subgroup levels of interest (e.g., by males and females separately).

In this work we assume heterogeneous treatment effects are induced by the existence of a detrimental subgroup $H$ with true marginal hazard ratio $\hplim > 1$ where the size of $H$ is at least $60$ subjects with an underlying expected event count $d$.  In our context there are two type-1 error scenarios for false subgroup identification: (i) If a subgroup $H$ is identified where in truth $\theta^{\dagger}(H) \leq 1$ (non-detrimental); and (ii) If the treatment effect is uniformly beneficial, $\hplimitt < 1$.   Under scenario (i) it is possible for 
heterogeneous treatment effects to exists (via mixture of true $H$ and $H^{c}$), but the composition of the identified subgroup $H$ is such that treatment is non-detrimental for the sub-population ($\hplim \leq 1$); in contrast under (ii) there does not exist such subgroup effects.
In the following section (\ref{sec:asymptotics}) we represent hazard ratio estimators based on a subgroup, and random splits thereof, via two normal random variables where the joint probability of meeting the screening and splitting consistency criterion thresholds is calculated by numerical integration.  Specifically, let $W_1$ and $W_2$ be two (independent) $N(\log(\hplim),8/d)$ random variables and define $p(c_1,c_2; d, \hplim) = \Pr(W_1+W_2 \geq 2 \log(c_1), \min(W_1,W_2) \geq \log(c_2))$ where $c_1$ and $c_2$ are the screening and consistency thresholds.  Here $W_1$ and $W_2$ represent the Cox estimators corresponding to the random (50/50) subgroup splits, and the sum $W_1 + W_2$ represents the Cox estimator for the subgroup.   For fixed $d$ and thresholds $\{c_1,c_2\}$ the type-1 error is approximately $p(c_1,c_2; d, 0)$ for $\hplim = 1$, and power $p(c_1,c_2; d, \hplim)$ for $\hplim > 1$.  The practical ramifications for false identification depends on the true $\hplim$.  For example, if the true treatment effect is uniform with an ITT benefit of $0.75$, which may be considered ``clinically significant'' in various oncology settings; then for subgroup size $n=60$ with a censoring rate of $45\%$ the type-1 error is approximately $4.9\%$ for $c_1=1.25$ and $c_2=1.0$ under $\hplim \approx \hplimitt = 0.75$ (details are discussed in Section \ref{sec:asymptotics}).

Now, we assume the candidate subgroups formed by combinations of ($p$-dimensional) baseline covariates $Z$ can be generated by $K$ categorical factors $\{X_{k}, k=1,\ldots,K\}$.  This imposes no restriction on covariates that are naturally categorical, and for continuous covariates any well-defined algorithm can be applied to select various cuts.  The FS procedure for identifying $H$ is implemented as follows.


\begin{enumerate}[{Step 1}(a)]
\item{For candidate baseline factors $X_{k}$, $k=1,\ldots,K$, construct dummy indicators for each unique factor level: Let $l_{k}$ denote the unique number of values with $L=\sum_{k=1}^{K}l_{k}$ the 
number of possible single factor subgroups.  For example, if $X_{1}$ denotes age cut at 50 years and $X_{2}$ denotes gender then $L=4$ ($\hbox{age} \leq 50$, $\hbox{age}>50$, $\hbox{gender=male}$, $\hbox{gender=female}$).}
\item{Let $J_{1},\ldots,J_{L}$ denote the resulting subgroup indicators. For example, for age cut at 50 years, $J_{1}=I(\hbox{age} \leq 50)$ indicates membership in the ``50 and younger'' 
subgroup; and $J_{2}=I(\hbox{age} > 50)$ indicates membership in the ``older than 50'' subgroup.  Each $J_{1},\ldots,J_{L}$ and (non-null) combinations between (e.g., ``males 50 and younger'') represents a potential subgroup.}
\end{enumerate}
\begin{enumerate}[{Step 2}]
\item{There are $2^{L}-1$ all-possible subgroup combinations where we restrict to those based on at most two factors.  The total number of possible two-factor combinations is ${L \choose 2} + L = L(L-1)/2+L$. As a minimal sample size criteria we further restrict to candidate subgroup combinations with a minimum size of $60$ subjects and with a minimum number of $10$ events in each treatment arm.  Let $\{G_{s}, s=1,\ldots, S\}$ denote the collection 
of subgroups meeting the sample size criteria where $S \leq L(L-1)/2+L$.}
\end{enumerate}
\begin{enumerate}[{Step 3}(a)]
\item{For subgroup $G_{s}$ (of size $\geq 60$ and at least 20 events), estimate the Cox model log-hazard ratio $\hat\beta_{s}$ (say), and consider the subgroup as a candidate if $\hat\beta_{s} \geq \log(1.25)$:}
\item{To judge the “consistency with harm”, randomly split the $G_{s}$ subgroup 50/50 and estimate the log-hazard ratio in each of these 2 random splits.
Consider this subgroup to be “consistent with harm” if, for each random split, both splits have estimated log-hazard ratios $\geq \log(1.0)$.  That is, 
$\min(\hat\beta_{s}^{1},\hat\beta_{s}^{2}) \geq \log(1.0)$ for log-hazard ratio estimate pairs $\{\hat\beta_{s}^{1}, \hat\beta_{s}^{2}\}$ corresponding to each random split;}
\item{Repeat many times (e.g., $R=400$) to estimate the consistency rate.  Let $\{\hat\beta_{s}^{1r}, \hat\beta_{s}^{2r} \}$ denote pairs for the r'th random split for $r=1, \ldots, R$ .  The consistency rate is then 
$$\hat{p}_{consistency}= \frac{1}{R}\sum_{r=1}^{R}\left\{I(\min(\hat\beta_{s}^{1r}, \hat\beta_{s}^{2r}) \geq 0) \right\}.$$
}
\end{enumerate}
\begin{enumerate}[{Step 4}]
\item{For subgroups with consistency rates at least $90\%$, choose the subgroup with the highest consistency rate as the estimated $H$, $\widehat{H}$ (''maximally consistent'');
if no subgroup achieves consistency $\geq 90\%$ then consider $H$ as null ($\widehat{H}=\emptyset$).  For the complementary group, $H^{c}$ is estimated as the complement of $\widehat{H}$, denoted $\widehat{H}^{c}$; if $\widehat{H}$ is null, then $\widehat{H}^{c}$ is the ITT population.
}
\end{enumerate}


In Step 4 the subgroup with the highest consistency rate is chosen, heuristically representing “no matter how you split the subgroup $\widehat{H}$, those splits are (generally) consistent with harm”.  This puts 
emphasis on maximizing the consistency rate.  To enable additional flexibility, Step 4 can be augmented or modified straightforwardly in several ways: (A) The inclusion of a median threshold, for the experimental arm, 
the control arm, or both.  For example one can restrict to subgroups wherein the experimental arm median is estimable and is below a clinically relevant value (e.g., 3-months); and/or (B) Instead of maximizing the consistency rate, emphasis 
on larger (or smaller) subgroups can be incorporated by selecting the largest (or smallest) subgroup among those with a high degree of consistency (e.g., at least $90\%$).

In our applications we illustrate approaches for identifying harmful, and strongly beneficial subgroups.


<<echo=FALSE,message=FALSE,warning=FALSE>>=
# source file in asymptotics subdirectory
load("results-input//pAnyH_approx_nsg.Rdata")
@

<<echo=FALSE,message=FALSE,warning=FALSE,eval=TRUE>>=
mdd.60<-min(hrH.plims[pAnyH.approx.60>=0.80])
mdd.80<-min(hrH.plims[pAnyH.approx.80>=0.80])
mdd.100<-min(hrH.plims[pAnyH.approx.100>=0.80])

# e for type-1 error
Edd.60<-max(hrH.plims[pAnyH.approx.60<=0.10])
Edd.80<-max(hrH.plims[pAnyH.approx.80<=0.10])
Edd.100<-max(hrH.plims[pAnyH.approx.100<=0.10])

# Look at thetaH around 0.80 (considering beneficial)
loc.beni <- min(which(hrH.plims>=0.75))
beni_hr <- round(hrH.plims[loc.beni],2)
alpha.beni.60 <-pAnyH.approx.60[loc.beni]
alpha.beni.80 <-pAnyH.approx.80[loc.beni]
alpha.beni.100 <-pAnyH.approx.100[loc.beni]
@


<<echo=FALSE,message=FALSE,warning=FALSE,eval=FALSE>>=
# For paper
# Original version, but revise below
# create eps by export function
#pdf(file = "Figure 2.pdf",   
#width = 10, height = 10) 
library(latex2exp)
plot(hrH.plims,pAnyH.approx.60,xlab="Hazard ratio for subgroup H",ylab="Probability any H found",type="l",lty=1,lwd=2.0,ylim=c(0,1))
abline(h=0.10,lwd=0.5,col="red")
lines(hrH.plims,pAnyH.approx.80,type="l",lty=1,lwd=2,col="grey")
lines(hrH.plims,pAnyH.approx.100,type="l",lty=1,lwd=2,col="blue")
#legend("top",
#legend=TeX(sprintf(r'($n_H = %d$)',c(60,80,100))),
#lty=1,lwd=2,col=c("black","grey","blue"),bty="n")
legend("topleft",legend=c("n = 60","n = 80","n = 100"),lty=1,lwd=2,col=c("black","grey","blue"),bty="n")
#dev.off()
@


<<echo=FALSE,message=FALSE,warning=FALSE,eval=FALSE>>=
#### For presentation slides ####
pdf(file = "plot2_approxH.pdf",   
width = 10, height = 10) 
plot(hrH.plims,pAnyH.approx.60,xlab="Hazard ratio for subgroup H",ylab="Probability of any H found",type="l",lty=1,lwd=2.0,ylim=c(0,1))
abline(h=0.10,lwd=0.5,col="red")
lines(hrH.plims,pAnyH.approx.80,type="l",lty=1,lwd=2,col="grey")
lines(hrH.plims,pAnyH.approx.100,type="l",lty=1,lwd=2,col="blue")
legend("top",legend=c("n=60","n=80","n=100"),lty=1,lwd=2,col=c("black","grey","blue"),bty="n")
dev.off()
@


\subsection{Asymptotic considerations for selecting screening and consistency thresholds} \label{sec:asymptotics}

We now describe how the power for identifying $H$ can be approximated.  That is, if a subgroup H exists with underlying (marginal) hazard ratio $\hplim$ corresponding to harm, then what is the chance of jointly meeting the screening and consistency criteria?  We denote the log-hazard ratio generically by $\beta$.  Let $L_{d}(\beta)$ denote the Cox score statistic based on subgroup $G_{s}$ of Step 3 with a total number of $d$ observed events and corresponding log-hazard ratio estimate $\hat\beta_{s} \geq \log(1.25)$ (according to Step 3).  

For the random splitting step of the FS algorithm form $\tilde{L}_{d}(\beta)=L_{d_1}(\beta)+L_{d_2}(\beta)$ where $L_{d_1}(\beta)$ and $L_{d_2}(\beta)$ are based on randomly generating an artificial stratification factor (a random binomial with probability $1/2$) with $\tilde{L}_{d}(\beta)$ the Cox score statistic based on the artificial stratification.   
Denote the Cox model estimates based on the above random splits by $\hat\beta_{s}^{1}$, and $\hat\beta_{s}^{2}$, respectively. Due to the purely random splitting $\hat\beta_{s} \approx \tilde\beta_{s}$ where $\tilde\beta_{s}$ is the (randomly) stratified Cox estimate. 
Applying the normal approximation for the log-hazard ratio we have $\hat\beta_{s}$ is approximated by $(4/d)\tilde{L}_{d}(0)$\cite{JT_1984}, which in turn is approximated in distribution by a $N(\beta,4/d)$ random variable.\cite{JT_1984}  Similarly, $\hat\beta_{s}^{1}$ and $\hat\beta_{s}^{2}$ are each independently approximated via $(8/d)L_{d_1}(0) \approx  N(\beta,8/d)$, and $(8/d)L_{d_2}(0) \approx  N(\beta,8/d)$, since for both random splits $d_1 \approx d_2 \approx d/2$. Write these approximations as $L_{d_1}(0) \approx (d/8)\hat\beta_{s}^{1}$, $L_{d_2}(0) \approx (d/8)\hat\beta_{s}^{2}$, and $\tilde{L}_{d}(0) \approx (d/4)\hat\beta_{s}$.  Now, by construction $\tilde{L}_{d}(0) = L_{d_1}(0)+L_{d_2}(0)$ and we thus have, approximately

\begin{equation}
{\hat\beta}_{s} \geq \log(1.25) \iff \hat\beta_{s}^{1}+\hat\beta_{s}^{2} \geq 2\log(1.25).
\label{split_approx}
\end{equation}

For a subgroup $H$ with underlying log-hazard ratio $\beta$ we can thus approximate the probability of identifying $H$ via 
$P(W_1+W_2 \geq 2\log(1.25), \min(W_1,W_2) \geq \log(1.0)) =$


\begin{equation}
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} I(w_{1}+w_{2} \geq 2\log(1.25))I(w_{1} \geq 0)I(w_{2} \geq 0) \varphi(w_{1};\beta,8/d) \varphi(w_{2};\beta,8/d) dw_{1}dw_{2}, 
\label{pHat_approx}
\end{equation}
\noindent
where $\{W_1,W_{2}\} \sim N(\beta,8/d)$ (independently), and 
$\varphi(\cdot;\beta,8/d)$ denotes the normal density with mean $\beta$ and variance $8/d$.  In Supplementary materials \textcolor{blue}{S1} we provide a simulation evaluation of 
the approximations in (\ref{split_approx}) and (\ref{pHat_approx}) where we find the approximations to appear quite accurate. 


\begin{figure*}
\begin{center}
\includegraphics[width=5.5in, height=3.25in]{Figure 2.eps}
\end{center}
\caption{Approximate probability of finding H via FS: Subgroup $H$ of size $n = 60, 80, 100$ with underlying hazard-ratio varying from 0.5 to 3.0 and with average censoring rate of $45\%$ so that $d \approx {0.55}n_{H}$.  The horizontal line indicates $10\%$. Approximately $80\%$ reached at underlying hazard-ratios: $\Sexpr{round(mdd.60,2)}$, $\Sexpr{round(mdd.80,2)}$, and $\Sexpr{round(mdd.100,2)}$, for $n = 60$, $80$, and $100$, respectively. \label{fig:ApproxH}}
\end{figure*}


Figure {\ref{fig:ApproxH}} displays (\ref{pHat_approx}) for scenarios where a subgroup $H$ exists (size $n=$ 60, 80, or 100) with underlying hazard-ratio $\hplim$ ranging from $0.5$ to $3.0$.
Hazard ratios $\leq 1.0$ correspond to non-negative treatment effects and the horizontal line is at $10\%$ suggesting the type-1 error rate is reasonable (with a sharp decline for $\hplim \leq 0.75$).  The power also seems reasonable, generally $\geq 70\%$ for identifying underlying hazard ratios in the $\geq 2.0$ range.  

Our choice of the $1.25$ and $1.0$ thresholds was based on the desire to control the rate for finding a subgroup $H$ to be $\approx 10\%$ when the underlying hazard ratio for $H$ is below 1.0. 
If the underlying treatment effect is uniform and beneficial then for a random subgroup $H$, Cox model estimates will randomly fluctuate around the ITT effect.  For example, for $\hplim \equiv \hplimitt = \Sexpr{beni_hr}$, the above approximation is $\Sexpr{round(alpha.beni.60,3)}$, $\Sexpr{round(alpha.beni.80,3)}$, and $\Sexpr{round(alpha.beni.100,3)}$ (for $n=60$, $80$, and $100$, respectively) indicating reasonable control of type-1 error.  
We note that because FS seeks subgroups with evidence for harm (viz-a-viz the screening and consistency thresholds) the chance of forming subgroups under the null with an estimated benefit randomly in favor of control is less likely the stronger the (uniform) ITT treatment effect.

In Supplementary materials \textcolor{blue}{S1} we provide the power approximations for censoring rates of $0\%$ and $80\%$ (The type-1 error decreases and power increases as the censoring rate decreases.).  In the following simulations we evaluate the type-1 error for falsely identifying a non-existent $H$, and power for subgroup identification under various scenarios designed to mimic potential Phase 2 and Phase 3 trial conditions.
