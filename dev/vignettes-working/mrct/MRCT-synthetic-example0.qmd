---
title: "Subgroup Identification Analysis"
subtitle: "Applications to consistency analyses in MRCTs"
author: "Larry Leon"
toc: true
number-sections: true
toc-depth: 3
format: 
  html:
    self-contained: true
    code-fold: true
    bibliography: references.bib
    fig_caption: yes
    keep_tex: yes
    latex_engine: xelatex
    citation_package: natbib
    fig_width: 10
    fig_height: 8
    fontsize: 10pt
    extra_dependencies: "subfig"
---


```{r Setup, message=FALSE}
# Set options
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.align = 'center',
  fig.retina = 2
)
rm(list=ls())
library(tinytex)
library(ggplot2)
library(table1)

library(gt)

# test these packages
# just for testing any conflicts
# library(tidyverse)
# library(plyr)
# library(dplyr)
# library(glmnet)

library(survival)
library(data.table)
library(randomForest)
library(grf)
library(policytree)
library(DiagrammeR)

library(grid)
library(forestploter)
library(randomizr)

# library(devtools)
# install_github("larry-leon/weightedsurv", force = TRUE)

#install.packages("weightedsurv")


# install_github("larry-leon/forestsearch", force = TRUE)


library(forestsearch)
library(weightedsurv)

#help(forestsearch)

#help(generate_aft_dgm_flex)

# Set theme for plots
theme_set(theme_minimal(base_size = 12))

```


# Summary

We evaluate the potential for utilizing subgroup analyses for enhancing regional consistency analyses in oncology multi-regional clinical trials (MRCTs).   We consider a setting 
where asia pacific (AP) countries consist of approximately $15-20\%$ of a global trial and the criteria for consistency is a positive point estimate (Cox model hazard-ratio $< 1.0$) while 
the global trial meets statistical significance.   

Suppose the following -- 

**ITT meets statistical significance but the AP regional analysis appears problematic (hazard-ratio estimate $\approx 1.0$)**

Can a subgroup with enhanced benefit be identified in the non-AP population which then translates to the AP population?   

That is, suppose there exists a subgroup ${\cal G}$ such that the treatment affect 
is more pronounced and (standard) Cox model analyses based on the ${\cal G}$ subgroup of AP (AP$({\cal G})$, say) enables meeting consistency.

We assume Cox model analyses are the primary analysis where only the treatment arm is included as a covariate (with or without stratification by randomization factors). 

While there are a plethora of subgroup identification approaches available we consider the *forestSearch* approach, @Leon_2024, which was developed for identifying strong subgroup treatment effects (i.e., strong HTEs).  

We consider three objectives:  (A) Identify a subgroup with the largest detrimental effect (*harm*, say) defined in terms of the largest hazard-ratio estimate exceeding a clinical threshold in favor of control (e.g., $\hat\beta \geq \log(0.90)$);  (B) Identify the *smallest* subgroup with a hazard-ratio estimate indicative of *harm* (e.g., $\hat\beta \geq \log(0.90)$); and (C) Identify the *largest* subgroup with strong benefit (e.g., $\hat\beta \leq \log(0.60)$).
These objectives are related with the overarching goal of identifying a subgroup with enhanced benefit;   In (A) and (B) we identify a subgroup indicative of *harm*, $\hat{\cal H}$ say, in which case the complementary subgroup (${\hat{\cal H}}^{c}:= {\hat{\cal G}}$) may be generally beneficial.  Whereas in (C) we are more directly *targeting* a strong (clinically compelling) beneficial subgroup effect.  

In this note we provide a brief review of the Weibull model in Section 2 where we describe a (2-phase) spline model that we utilize for inducing differential treatment effects by way of a ''biomarker''.  Section 3 provides examples of how to generate ''biomarker profiles'' of interest where one can define subgroups based on biomarker cutpoints which correspond to underlying causal hazard ratio effects (e.g., causal hazard-ratio effect for biomarker $\geq 2$ is approximately $0.5$, say).  Section 4 describes the simulation setup (data generating model/process) and generates an example dataset used for analysis illustration where in Section 5 we discuss details of implementing (A)-(C) above.  In Section 6 we evaluate the operating characteristics for objective (B) under conditions where there exists a strong biomarker effect, and under the null of a uniform treatment benefit (i.e., ${\cal G} = \emptyset$).   Lastly, Section 7 briefly discusses estimation (de-biased point and CI estimates) -- taking into account the identification algorithm -- where there is interest in inference regarding the population from which the subgroup was identified (e.g., non-AP).  

Throughout this note R code is provided and illustrated based on real trial data (subject-level baseline characteristics are fixed at their observed quantities).  Our hope is that these notes can serve as a framework for future trial planning as well as for simulating from study data to understand the operating characteristics for teams and external discussions.

A diagram illustrating the general flow follows.


```{r}
#| fig-cap: "Subgroup identification analysis based on non-AP data to identify subgroups of marked benefit in AP. Operating characteristics evaluates whole process starting from analysis option"
#| label: analysis-diagram
library(png)
library(grid)
img <- readPNG('MRCT-Flow.png')
grid.raster(img)
```



# Weibull AFT/Cox model with biomarker effects

## Brief Review
For the Weibull distribution with shape parameter $\nu$ and scale parameter $\theta$ the
density, cdf, survival, hazard and cumulative hazard functions are:
\begin{eqnarray*}
f(t) &=& \nu \theta^{-\nu}t^{\nu-1}\exp(-(t/\theta)^{\nu}), \cr F(t)
&=& \int_{0}^{t}\nu \theta^{-\nu}s^{\nu-1}\exp(-(s/\theta)^{\nu})ds
\cr &=& \int_{0}^{(t/\theta)^{\nu}}e^{-w}dw \cr & &
(w=(s/\theta)^{\nu},dw=\nu s^{\nu-1}\theta^{-\nu}ds) \cr
&=&1-\exp(-(t/\theta)^{\nu}), \cr S(t) &=& \exp(-(t/\theta)^{\nu}),
\cr \lambda(t)&=&\nu \theta^{-\nu}t^{\nu-1}, \cr \Lambda(t) &=&
-\log(S(t))=(t/\theta)^{\nu}.
\end{eqnarray*}
Note that here we define the density to correspond with R's definition.
For shape parameter $\nu \in (0,1)$ the hazard is strictly decreasing in $t \geq 0$,
whereas for $\nu >1$ the hazard is strictly increasing in $t \geq 0$.  

*Note*: $\Lambda(T) \sim E(1)$

The cumulative hazard function $\Lambda(\cdot)$ evaluated at $T$, $\Lambda(T)$, as a random variable has cdf
$$\eqalign{ \Pr(\Lambda(T) \leq t) &=\Pr(-\log(1-F(T)) \leq t) =\Pr(1-F(T) \geq e^{-t}) \cr
&=\Pr(T \leq F^{-1}(1-e^{-t})) =F(F^{-1}(1-e^{-t}))  = 1-e^{-t}, \cr}$$
\noindent
which is the CDF of the exponential distribution, $E(1)$ (say). 


In the following we use 

\begin{equation}
\tag{1}
\Lambda(T) \sim E(1)
\end{equation}
\noindent
to represent the Weibull regression model as an AFT model which is also a Cox model. 

Now, $\Lambda(T)=(T/\theta)^{\nu}$ and write
$Q=-\log(S(T))=\Lambda(T)=(T/\theta)^{\nu}$, where from (1), $Q \sim E(1)$.  That is
$\log(Q)=\nu(\log(T)-\log(\theta))$ can be expressed as

\begin{equation}
\tag{2}
\log(T)=\log(\theta)+ \tau \log(Q) := \log(\theta) + \tau \epsilon,
\end{equation}
\noindent
where $\tau=1/\nu$ and it is easy to show that $\epsilon=\log(Q)$ has the ``extreme
value'' distribution with density $f_{\epsilon}(x)=\exp(x-e^{x})$ for $x \in {\cal R}$.
Here the range of $\log(T) \in {\cal R}$ is un-restricted.  The *survReg* routine uses the parameterization $(2)$ and therefore estimates
$\log(\theta)$ and $\tau=1/\nu$.

To incorporate covariates $L$ (say) we specify
$$\lambda(t;L)=\big(\nu \theta^{-\nu}t^{\nu-1} \big) \exp(L'\beta)
:= \lambda_{0}(t)\exp(L'\beta),$$
\noindent
where $\lambda_{0}(t)$ is the hazard, say, for $T_{0} \sim \hbox{Weibull}(\nu,\theta)$.
This is a special case of the proportional hazards model. The chf (conditional chf with covariate vector
$L$) is $\Lambda(t;Z)=(t/\theta)^{\nu}\exp(L'\beta)$ so that
analogous to above this leads to the representation

\begin{equation}
\tag{3}
\log(T) =\log(\theta)+\tau[-L'\beta+\epsilon] =\log(\theta)+L'\gamma + \tau \epsilon,
\end{equation}
\noindent
where $\gamma=-\tau\beta$, with $\tau$ and $\epsilon$ defined in (2).  R *survReg* uses this AFT parameterization so that the estimated components of
$\gamma$, $\gamma_{p}$ say, are that of $-\tau\beta_{p}$ for $p=1,\ldots,m$ ($m$ is dimension of $\beta$).

When fitting the AFT model (3) via suvreg we therefore transform parameters $\hat\gamma$ to the Weibull hazard-ratio parameterization (2) via

\begin{equation}
\tag{4}
\hat\beta = -\hat\gamma / \hat{\tau}.
\end{equation}


As an illustration we compare the *survReg* model fits for the case-study dataset.  The following table below compares the Weibull *survReg* model fits with covariates Treat and Ecog1 (Ecog = 1 vs 0) where components of $\hat\gamma$ from model (3) are calculated according to *survReg* and $\hat\beta$ are formed via (4).  In the table below Weibull estimates of $\hat\beta$ are compared to Cox model versions.
  
# Case-study

For illustration we use a synthetic case-study dataset that is based on a clinical trial with characteristics `perturbed` and masked.

The case-study dataset will be the basis for simulating subgroup effects generated by either binary factors, continuous covariates (e.g., biomarkers), or a combination.

This will serve as a framework for simulating from actual study data to evaluate operating characteristics of post-hoc analyses that may be conducted to understand `subgroup type` questions.

The objectives here are to evaluate the potential for utilizing subgroup identification methods to identify subgroup with `enhanced benefit` to inform regional consistency objectives of MRCTs as described in the introduction.
  
```{r Case-study}
# case-study example
# Draw sample from synthetic dataset (n=2,000)
dfsynth <- read.table("../data/dfsynthetic.csv", header=TRUE, sep=",")
df.case <- dfsynth
# Create "Asia Pacific (AP)" region flag variable
df.case$AP <- df.case$region_asia
```

## Kaplan-Meier curves of synthetic dataset

Note that the outcomes are representative of the trial data

```{r, fig.width = 7, fig.height = 5}
# Restrict to 1st 500
#df.case0 <- subset(df.case, pid <= 500)

dfcount <- df_counting(
  df = df.case,
  by.risk = 6,
  tte.name = "tte", 
  event.name = "event", 
  treat.name = "treat"
)
plot_weighted_km(dfcount, conf.int = TRUE, show.logrank = TRUE, ymax = 1.05, xmed.fraction = 0.775, ymed.offset = 0.125)
```


```{r}
df.case$bm5 <- ifelse(df.case$bm >= 5, 1,0)

create_summary_table(data = df.case, treat_var="treat", 
                     table_title = "N=500 Characteristics by Treatment Arm",
                                      vars_continuous=c("age","bm","entrytime"),
                                      vars_categorical=c("male","histology","prior_treat","AP","ecog","strat","bm5"),
                                      font_size = 12)


```



Illustration of correspondence between Weibull model and Cox

```{r Weibull-vs-Cox}
library(dplyr)
# Comparing Weibull vs Cox with case-study 
# This is just for illustration to show conversion of Weibull parameters from 
# AFT regression to Weibull hazard 
fit.weib_ex <- survreg(Surv(tte,pmax(event,1)) ~ treat + ecog, dist='weibull', data=df.case)
tauhat <- fit.weib_ex$scale
# convert (treat,ecog1) regression parms to Weibull hazard-ratio
bhat.weib <- -(1)*coef(fit.weib_ex)[c(2,3)]/tauhat
# Compare to Cox 
fit.cox_ex <- coxph(Surv(tte,pmax(event,1)) ~ treat + ecog, data=df.case)
res <- cbind(bhat.weib,coef(fit.cox_ex))
res <- as.data.frame(res)
colnames(res)<-c("Weibull","Cox")
res |> gt() |>
fmt_number(columns=1:2,decimals=6) |>
tab_options(
    table.font.size = px(12)
  ) |>
tab_header(title="Comparing Weibull to Cox hazard ratio estimates",
subtitle="Case-study dataset with artificial outcomes")

```




### Overview

The `generate_aft_dgm_flex()` function creates a flexible data generating mechanism (DGM) for survival data that can incorporate complex, non-linear covariate effects through spline specifications. This is particularly useful for simulating realistic clinical trial scenarios where biomarker effects vary across their range.

### Example: Creating a Spline-Based DGM

In this example, we create a DGM where the effect of a biomarker (`bm`) varies non-linearly, exhibiting different hazard ratios across three segments of the biomarker distribution.
```{r create-dgm-spline}
dgm_spline <- generate_aft_dgm_flex(
  data = df.case,
  # Main effects: continuous and factor variables
  # NOTE that ALL variables in the outcome model will have a "z_" prefix attached
  continuous_vars = c("age", "bm"),
  factor_vars = c("male", "histology", "prior_treat", "AP"),
  # Force specific coefficient for AP variable
  set_beta_spec = list(
    set_var = c("z_AP"),
    beta_var = -log(5)  # HR = 0.2 (strong protective effect)
    ),
  # Censoring mechanism covariates
  # NOTE that ALL variables in the censoring model will have a "zcens_" prefix attached
  # See notes below on how the censoring model is selected
  continuous_vars_cens = c("age"),
  factor_vars_cens = c("prior_treat"),
  cens_type = "weibull",
  # Outcome specification
  outcome_var = "tte",
  event_var = "event",
  treatment_var = "treat",
  # No additional subgroup factors 
  subgroup_vars = NULL,
  subgroup_cuts = NULL,
  k_inter = 0.0,
  # indicate that subgroup effects are incorporated
  model = "alt",
  # Spline specification for non-linear biomarker effect
  spline_spec = list(
    var = "z_bm",                    # Variable with non-linear effect
    knot = 5,                        # Spline knot position
    zeta = 10,                       # Transition smoothness parameter
    log_hrs = log(c(2, 1.25, 0.5))  # Hazard ratios for the three segments
  ),
  verbose = TRUE,
  standardize = FALSE #default
)
```


### Notes

- The `z_` prefix in variable names (e.g., `z_bm`, `z_AP`) indicates the original variables that are true factors in the outcome model when standardize=false (if true then the variables
are standardized prior to estimation)
- Setting `k_inter = 0.0` turns off any treatment-subgroup interaction effects unless the spline model is implemented
- The `verbose = TRUE` option provides detailed output during DGM creation
- The `model = "alt"` specifies whether the true outcome model has heterogeneous treatment effects (HTEs)

For the **censoring distribution** with `cens_type ="weibull"`, `continuous_vars_cens = c("age")`, and 
`factor_vars_cens = c("prior_treat")` four censoring models will be estimated using weibull and log-normal distributions including treatment with 
the specified variables as well as no covariates.  That is, if a weibull model without covariate adjustment is found to provide a better fit (per AIC/BIC) 
then this will be selected. NOTE that the censoring distribution is completely independent of the outcome model and so this aspect does not depend on how 
the survival times are generated.
  

#### Examine beta = log(hr) parameters

Verify that the effect for "z_AP" is set to the specified value (Notice the "z_" prefixes)

Note that treatment effects with interactions (e.g., spline and/or subgroups) are interpreted below.

```{r}
library(dplyr)
b0_coefs <- dgm_spline$model_params$b0

coef_df <- data.frame(
  Coefficient = names(b0_coefs),
  Value = as.numeric(b0_coefs),
  HR = exp(as.numeric(b0_coefs))
) |>
  mutate(
    Type = case_when(
      grepl("^treat", Coefficient) ~ "Treatment Effects",
      grepl("^z_bm", Coefficient) ~ "Biomarker (Spline)",
      grepl("^z_", Coefficient) ~ "Baseline Covariates",
      TRUE ~ "Other"
    )
  )

coef_table_grouped <- coef_df |>
  arrange(Type, Coefficient) |>
  gt(groupname_col = "Type") |>
  fmt_number(
    columns = c(Value, HR),
    decimals = 4
  ) |>
  tab_header(
    title = "AFT Model Coefficients by Type",
    subtitle = "Weibull Parameterization (b0) with Hazard Ratios"
  ) |>
  cols_label(
    Coefficient = "Variable",
    Value = "b0 Coefficient",
    HR = "Hazard Ratio"
  ) |>
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) |>
  tab_style(
    style = cell_text(style = "italic"),
    locations = cells_row_groups()
  ) |>
  tab_options(
    table.font.size = px(12)
  )



coef_table_grouped

# Confirm forced parameter value
#cat("AP region parmeter = -log(5)?", c(dgm_spline$model_params$b0["z_AP"],-log(5)),"\n")
```




### Understanding the Spline Specification

The `spline_spec` parameter creates a **three-segment piecewise effect** for the biomarker:

- **Segment 1** (low biomarker values): HR = 2.0 (harmful effect)
- **Segment 2** (intermediate values): HR = 1.25 (modest harmful effect)  
- **Segment 3** (high biomarker values): HR = 0.5 (protective effect)

The transition between segments occurs smoothly around `knot = 5`, with the degree of smoothness controlled by `zeta = 10`. Higher values of `zeta` create sharper transitions between segments.

See log(hazard-ratio) plot below.


View the censoring parameters:  

In this case example the best fitting censoring model is a common censoring model (weibull with NO covariates).   

If the selected model included 
covariates then these would show up with "zcens_" prefixes attached.   If `cens_type="uniform"` then a uniform model is implemented.   

```{r}
dgm_spline$model_params$censoring
```


### What is Created

The `dgm_spline` object contains:

1. **Linear coefficients** for all specified covariates (age, male, histology, prior_treat, AP)
2. **Spline parameters** defining the non-linear effect of the biomarker
3. **Censoring distribution parameters** (Weibull/lognormal with specified covariates, or uniform)
4. **Model structure** ready for use with `simulate_from_dgm()`

**Causal log-hazard-ratio**

Recall, the log(hazard-ratio) for biomarker level $z$ `="z_bm"` is a linear function of $z$ with a change-point (in slope) at $z=k$ given by 

\begin{eqnarray*}
\psi^{0}(z) &=& \beta^{0}_{1} + \beta^{0}_{3}z, \quad \hbox{for} \ z \leq k, \cr
            &=& \beta^{0}_{1} + \beta^{0}_{3}z + \beta^{0}_{5}(z-k), \quad \hbox{for} \ z > k.
\end{eqnarray*}

Log hazard-ratio parameters $(\beta^{0}_{1},\beta^{0}_{3},\beta^{0}_{5})$ can be chosen to generate "treatment effect patterns" by specifying $\psi^{0}(z)$ values at 
$z=0$, $z=k$, and $z=\zeta$ for $\zeta > k$.  For specified $\psi^{0}(0)$, $\psi^{0}(k)$, and $\psi^{0}(\zeta)$ we have 

\begin{eqnarray}
\beta^{0}_{1} &=& \psi^{0}(0), \cr
\beta^{0}_{3} &=& {(\psi^{0}(k) - \beta^{0}_{1}) \over k}, \cr
\beta^{0}_{5} &=& {(\psi^{0}(\zeta) - \beta^{0}_{1} - \beta^{0}_{3}\zeta) \over (\zeta -k)}.
\end{eqnarray}

In the above table $\beta^{0}_{1}$ corresponds to `treat`, $\beta^{0}_{2}$ to `z_bm`, $\beta^{0}_{3}$ to `z_bm_treat`, $\beta^{0}_{4}$ to `z_bm_k`, 
and $\beta^{0}_{5}$ to `z_bm_k_treat`.

Note that defining $m(1,z)$ [$m(0,z)$)] as the conditional expected value of $\log(T)$ with treatment set to experimental [control] given biomarker level $Z=z$  

$$\psi^{0}(z) = \{m(0,z)-m(1,z)\}/\tau$$
\noindent
which corresponds to the difference in the (true) means of the potential outcomes under experimental and control conditions ($\log(T[0,z]$ versus $\log(T[1,z]$, say). 


The **super-population** dataset `dgm_spline$df_super`:

Note that in `df_super`: `lin_pred_1` = $m(1,z)/\tau$, `lin_pred_0` = $m(0,z)/\tau$, and `loghr_po` = $\psi^{0}(z) = {m(0,z) - m(1,z)}/\tau$ 


```{r}
# Note that here flag_harm is not relevant because of the spline specification and definition of "harm" depends on degree
# In discussion below we describe how to specify thresholds of "harm" corresponding to biomarker effects
create_summary_table(data=dgm_spline$df_super, treat_var="treat", 
                     table_title = "Super-population Characteristics by Treatment Arm",
                                      vars_continuous=c("z_age","z_bm","entrytime","lin_pred_1","lin_pred_0","loghr_po"),
                                      vars_categorical=c("z_male","z_histology","z_prior_treat","z_AP","ecog","strat","region_other","flag_harm"),
                                      font_size = 12)


```





### Simulating Data from the DGM

Once created, the DGM can be used to generate synthetic datasets:

Here we generate n=500 retaining the treatment assigment of the case-study to mimic the randomization and utilize the actual entry times (if available).  

```{r simulate-from-dgm}
# Generate a dataset with n=500 subjects:
# Randomized 1:1 (rand_ratio=1);
# Retain the original treatment assignment in df.case (draw_treatment=FALSE)
# Note that draw_treatment=TRUE will draw randomized treatment assignments
# Use observed entry times per df.case if provided (entry_var = entrytime);
# Otherwise entry times are simulated as uniform(0, max_entry)
# Entry times are then incorporated in administrative censoring at the time 
# of analysis (eg., analysis_time = 60);
# That is, subjects with entrytime > analysis_time are not available
# and follow-up is "60 - entrytime" for each subjects entrytime value
# The cens_adjust argument allows for increasing or decreasing the mean of 
# the censoring distribution in order to reduce or increase the censoring rate
df_example <- simulate_from_dgm(
  dgm = dgm_spline,
  n = 500,
  rand_ratio = 1, 
  draw_treatment = FALSE,
  entry_var = "entrytime",
  analysis_time = 60,
  cens_adjust = 0,
  seed = 12345
)

# Examine the simulated data structure
head(df_example)
```
```{r}
# Note that y_sim, treat_sim, and event_sim are the outcomes, treatment, and events for the simulated data  
# t_true are the uncensored outcomes, and c_time are the uncensored (not censored by outcomes) censoring times
# Of course in reality t_true and c_time are not observable
create_summary_table(data = df_example, treat_var="treat_sim", 
                                      table_title = "Simulated data (n=500) from Spline Model",
                                      vars_continuous=c("y_sim","t_true","c_time","z_age","z_bm","entrytime","lin_pred_1","lin_pred_0","loghr_po"),
                                      vars_categorical=c("event_sim","z_male","z_histology","z_prior_treat","z_AP","ecog","strat","region_other","flag_harm"))

```


### Visualizing the Non-Linear Effect

The non-linear effect of the biomarker can be visualized by plotting the log hazard ratio across the range of biomarker values:

```{r plot-spline-effect,  fig.width = 7, fig.height = 5}
# Plot the spline effect
plot_spline_treatment_effect(dgm_spline)
```

A *more detailed* look at the spline causal effects: AHR, and CDEs

We draw a large trial (n=5000) as a large sample approximation to summarize the underlying `population` effects.

```{r, fig.width = 10, fig.height = 7}

# Large sample approximation

df_large <- simulate_from_dgm(
  dgm = dgm_spline,
  n = 5000,
  rand_ratio = 1, 
  draw_treatment = FALSE,
  entry_var = "entrytime",
  analysis_time = 60,
  cens_adjust = 0,
  seed = 12345
)


# hr_threshold = 1.25 specifies finding the biomarker level such that the AHR (loghr_po) is above 1.25

results <- cox_ahr_cde_analysis(
   df = df_large,
   tte_name = "y_sim", event_name = "event_sim", treat_name = "treat_sim", 
   z_name = "z_bm",
   hr_threshold = 1.25,
   plot_style = "grid"
 )



```

```{r}
 res <-results$subgroup_stats
 cut_hr <- results$optimal_cutpoint

res |>
  gt() |>
  tab_header(
    title = "Subgroup Statistics"
  ) |>
  tab_source_note(
    source_note = md(paste0("*HR threshold biomarker cutpoint: ", 
                            round(cut_hr, 3), "*"))
  ) |>
  # Format first 3 columns with 0 decimals
  fmt_number(
    columns = 1:4,
    decimals = 0
  ) |>
  # Format remaining numeric columns with 3 decimals
  fmt_number(
    columns = -c(1:4),
    decimals = 3
  ) |>
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) |>
  tab_options(
    table.font.size = px(12)
  )

```


Next, a simulated example (n=500) with forestsearch subgroup identification   

-  Here we simulated a study according to the biomarker effects described above
-  Summarize the non-AP and AP populations (K-M plots)  
-  Apply subgroup identification procedures to the non-AP data and apply to AP  


Returning to the simulated example data `df_example` (described above):  (a) View K-M curves; (b) Estimate spline model to compare with true biomarker effects; and (c) Apply forestsearch to identify subgroups base on non-AP data and applied to AP.


```{r, fig.width = 10, fig.height = 7}
# ITT 
dfc <- df_counting(
  df = df_example,
  by.risk = 6,
  tte.name = "y_sim", 
  event.name = "event_sim", 
  treat.name = "treat_sim"
)
# No censoring
# Except by max_follow
df_example$t_true <- pmin(df_example$t_true, 100)
df_example$event_mod <- ifelse(df_example$t_true <= 100,1,0)
dfideal <- df_counting(
  df = df_example,
  by.risk = 6,
  tte.name = "t_true", 
  event.name = "event_mod", 
  treat.name = "treat_sim"
)
df_nonAP <- subset(df_example, z_AP == 0)
df_AP <- subset(df_example, z_AP == 1)
# non-AP
dfc0 <- df_counting(
  df = df_nonAP,
  by.risk = 6,
  tte.name = "y_sim", 
  event.name = "event_sim", 
  treat.name = "treat_sim"
)

# AP
dfc1 <- df_counting(
  df = df_AP,
  by.risk = 6,
  tte.name = "y_sim", 
  event.name = "event_sim", 
  treat.name = "treat_sim"
)


par(mfrow=c(2,2))

plot_weighted_km(dfc, conf.int = TRUE, show.logrank = FALSE, ymax = 1, xmed.fraction = 0.75, ymed.offset = 0.15)
title("ITT censored")

plot_weighted_km(dfc0, conf.int = TRUE, show.logrank = FALSE, ymax = 1, xmed.fraction = 0.725, ymed.offset = 0.15)
title("non-AP")

plot_weighted_km(dfc1, conf.int = TRUE, show.logrank = FALSE, ymax = 1, xmed.fraction = 0.725, ymed.offset = 0.15)
title("AP")

```


```{r, fig.width = 8, fig.height = 5}

plot_weighted_km(dfc1, conf.int = FALSE, show.logrank = FALSE, ymax = 1, xmed.fraction = 0.725, ymed.offset = 0.15)
title("AP")

```




Spline fit to observed data

```{r, fig.width = 8, fig.height = 5}

result <- cox_cs_fit(
  df = df_example, tte_name = "y_sim", event_name ="event_sim", treat_name = "treat_sim",
  truebeta_name = "loghr_po",
  z_name = "z_bm", z_by = 1, z_window = 0.5,
  alpha = 0.20,
  plot_params = list(
    xlab = "Biomarker Level",
    main_title = "Treatment Effect Heterogeneity"
  )
)
```







# Subgroup identification criteria:  Options for targeting subgroup effects

## Finding Non-AP subgroup with highest consistency rate (in favor of control)  

In the following we evaluate subgroups based on single-factors (e.g., "biomarker < J" say, or "Age <= 65", etc) and 
select the subgroup with the highest consistency rate with Cox hazard-ratio estimate meeting selection criteria;  The selected subgroup with the highest 
consistency rate meeting hazard ratio estimate criterion (log hazard-ratio estimate $\log(\hat\beta) \geq \log(0.90)$, say) where the "consistency rate" is at least $90$\%.

Here we are seeking to identify subgroups where the hazard ratio estimate is at least $\log(0.90)$ which may indicate limited benefit.  Further, subgroup candidates are ranked according 
to highest hazard ratio estimate and highest consistency rate.


```{r fs1, echo=TRUE, message=FALSE, warning=TRUE, fig.width = 10, fig.height = 8, eval = TRUE}

confounders.name <- c("z_age","z_bm","z_male","ecog","z_histology","z_prior_treat","strat")

dfa <- as.data.frame(df_nonAP)

# Setup parallel processing
library(doFuture)
library(doRNG)

registerDoFuture()
registerDoRNG()

# Analysis dataset (estimation/training) is non-AP (df_nonAP)
# Testing dataset is AP (df_AP) --> predictions per non-AP analysis are applied to AP

system.time({fs <- forestsearch(dfa,  confounders.name=confounders.name,
                                outcome.name = "y_sim", treat.name = "treat_sim", event.name = "event_sim", id.name = "id",
                                potentialOutcome.name = "loghr_po", 
                                df.test = as.data.frame(df_AP),
                                flag_harm.name = NULL,
                                hr.threshold = 0.9, hr.consistency = 0.80, pconsistency.threshold = 0.80,
                                sg_focus = "minSG", max_subgroups_search = 30,
                                showten_subgroups = TRUE, details=TRUE,
                                conf_force = c("z_age <= 65", "z_bm <= 0", "z_bm <= 1", "z_bm <= 2","z_bm <= 5"),
                                cut_type = "default", use_grf = TRUE, plot.grf = TRUE, use_lasso = TRUE,
                                maxk = 1, n.min = 60, d0.min = 12, d1.min = 12,
                                plot.sg = TRUE, by.risk = 6,
                                parallel_args = list(plan="callr", workers = 100, show_message = TRUE)
)
})

plan("sequential")


# Results for estimation (training) data, which_df = "est" is default
res_tabs <- sg_tables(fs, ndecimals = 3, which_df = "est")

res_tabs$sg10_out

res_tabs$tab_estimates

# Results for testing dataset (AP)
res_tabs <- sg_tables(fs, ndecimals = 3, which_df = "testing", est_caption = "Testing (AP) dataset")
res_tabs$tab_estimates

```


Next, generate under `null` where treatment is uniform.  Here we set a constant biomarker effect such that hr = 0.7

```{r dgm-null, echo=TRUE, message=FALSE, warning=TRUE, fig.width = 10, fig.height = 8, eval = TRUE}

dgm_spline_null <- generate_aft_dgm_flex(
  data = df.case,
  continuous_vars = c("age", "bm"),
  factor_vars = c("male", "histology", "prior_treat","AP"),
  set_beta_spec = list(set_var = c("z_AP"), beta_var = -log(5)),
  continuous_vars_cens = c("age"),
  factor_vars_cens = c("prior_treat"),
  outcome_var = "tte",
  event_var = "event",
  treatment_var = "treat",
  cens_type = "weibull",
  subgroup_vars = NULL,
  subgroup_cuts = NULL,
  model = "alt",
  spline_spec = list(
    var = "z_bm",  # Use original variable name (or "z_er" if transformed)
    knot = 5,
    zeta = 10,
    log_hrs = log(c(0.7, 0.7, 0.7))  # HR varies with ER
  ),
  k_inter = 0.0,  # Amplify or turn-off (=0) subgroup per subgroup_vars effect?
  verbose = TRUE
)

```



```{r, fig.width=10, fig.height=7}

# Simulate data

# cens_adjust  - 3.5

df_example <- simulate_from_dgm(
  dgm = dgm_spline_null, 
  n = 500, 
  rand_ratio = 1, 
  draw_treatment = FALSE,
  entry_var = "entrytime",
  analysis_time = 60,
  cens_adjust = 0,
  seed = 12345
)

dim(df_example)
with(df_example,sum(event_sim))



# ITT
dfc <- df_counting(
  df = df_example,
  by.risk = 6,
  tte.name = "y_sim", 
  event.name = "event_sim", 
  treat.name = "treat_sim"
)

# No censoring
# Except by max_follow

df_example$t_true <- pmin(df_example$t_true, 100)

df_example$event_mod <- ifelse(df_example$t_true <= 100,1,0)

dfideal <- df_counting(
  df = df_example,
  by.risk = 6,
  tte.name = "t_true", 
  event.name = "event_mod", 
  treat.name = "treat_sim"
)


df_nonAP <- subset(df_example, z_AP == 0)
df_AP <- subset(df_example, z_AP == 1)


# non-AP
dfc0 <- df_counting(
  df = df_nonAP,
  by.risk = 6,
  tte.name = "y_sim", 
  event.name = "event_sim", 
  treat.name = "treat_sim"
)

# AP
dfc1 <- df_counting(
  df = df_AP,
  by.risk = 6,
  tte.name = "y_sim", 
  event.name = "event_sim", 
  treat.name = "treat_sim"
)


par(mfrow=c(2,2))

plot_weighted_km(dfideal, conf.int = FALSE, show.logrank = TRUE, ymax = 1.1, xmed.fraction = 0.75, ymed.offset = 0.15)
title("ITT no censoring (through 100 months)")

plot_weighted_km(dfc, conf.int = FALSE, show.logrank = TRUE, ymax = 1.1, xmed.fraction = 0.75, ymed.offset = 0.15)
title("ITT censored")

plot_weighted_km(dfc0, conf.int = FALSE, show.logrank = TRUE, ymax = 1.1, xmed.fraction = 0.725, ymed.offset = 0.15)
title("non-AP")

plot_weighted_km(dfc1, conf.int = FALSE, show.logrank = TRUE, ymax = 1.1, xmed.fraction = 0.725, ymed.offset = 0.15)
title("AP")


```




```{r fs-null, echo=TRUE, message=FALSE, warning=TRUE, fig.width = 10, fig.height = 8, eval = TRUE}


confounders.name <- c("z_age","z_bm","z_male","ecog","z_histology","z_prior_treat","strat")


dfa <- as.data.frame(df_nonAP)

# Setup parallel processing
library(doFuture)
library(doRNG)

registerDoFuture()
registerDoRNG()

system.time({fs <- forestsearch(dfa,  confounders.name=confounders.name,
                                outcome.name = "y_sim", treat.name = "treat_sim", event.name = "event_sim", id.name = "id",
                                potentialOutcome.name = "loghr_po", 
                                df.test = as.data.frame(df_AP),
                                flag_harm.name = NULL,
                                hr.threshold = 0.9, hr.consistency = 0.80, pconsistency.threshold = 0.80,
                                sg_focus = "minSG", max_subgroups_search = 30,
                                showten_subgroups = TRUE, details=TRUE,
                                conf_force = c("z_age <= 65", "z_bm <= 0", "z_bm <= 1", "z_bm <= 2","z_bm <= 5"),
                                cut_type = "default", use_grf = TRUE, plot.grf = TRUE, use_lasso = TRUE,
                                maxk = 1, n.min = 60, d0.min = 12, d1.min = 12,
                                plot.sg = TRUE, by.risk = 6,
                                parallel_args = list(plan="callr", workers = 100, show_message = TRUE)
)
})

plan("sequential")


```


# Appendix

## Biomarker effects with spline model  

We now outline how potential outcomes are simulated according to parameters fit to the case-study dataset but with parameters specified to induce biomarker effects.  That is, causal treatment effects (on log(hazard-ratio) scale) that follow 
a spline model according to patterns where biomarker effects increase with biomarker levels;  Including various degrees of limited treatment effects for low biomarker levels.

We first consider a Weibull model with treatment and a single biomarker covariate $Z$ where we write the linear predictor of the Cox model $L'\beta$ (say) as 

\begin{equation}
\tag{5}
L'\beta  := \beta_{1}\hbox{Treat} + \beta_{2}\hbox{Z} + \beta_{3}\hbox{Z}\hbox{Treat} + \beta_{4}(\hbox{Z}-k)I(\hbox{Z}>k) + \beta_{5}(\hbox{Z}-k)I(\hbox{Z}>k)\hbox{Treat}.
\end{equation}

Following the potential-outcome approach let $l_{x,z}$ denote subject's hazard-function "had they followed treatment regimen $Treat=x$ while having biomarker level $Z=z$".   That is, for subject with biomarker level $Z=z$ we can simulate 
their survival outcomes under both treatment ($x=1$) and control ($x=0$) conditions.   Let $\beta^{0} = (\beta_{1}^{0},\ldots,\beta_{5}^{0})'$ denote the true coefficients and denote the hazard function as 

$$\lambda_{x,z}(t) = \lambda_{0}(t)\exp(l_{x,z}), \quad \hbox{say}.$$  

Writing 

\begin{equation}
l_{x,z} = \beta^{0}_{1}x + \beta^{0}_{2}z + \beta^{0}_{3}zx + \beta^{0}_{4}(z-k)I(z>k) + 
 \beta^{0}_{5}(z-k)I(z>k)x,
\end{equation}
\noindent
the log of the hazard ratio for biomarker level $z$ under treatment ($x=1$) relative to control ($x=0$) is given by 

\begin{equation}
\tag{6}
\psi^{0}(z) := \log(\lambda_{1,z}(t)/\lambda_{0,z}(t)) = \beta^{0}_{1} + \beta^{0}_{3}z + \beta^{0}_{5}(z-k)I(z>k).
\end{equation}


## Causal log-hazard-ratio

The log(hazard-ratio) for biomarker level $z$ is a linear function of $z$ with a change-point (in slope) at $z=k$ given by 

\begin{eqnarray*}
\psi^{0}(z) &=& \beta^{0}_{1} + \beta^{0}_{3}z, \quad \hbox{for} \ z \leq k, \cr
            &=& \beta^{0}_{1} + \beta^{0}_{3}z + \beta^{0}_{5}(z-k), \quad \hbox{for} \ z > k.
\end{eqnarray*}

Log hazard-ratio parameters $(\beta^{0}_{1},\beta^{0}_{3},\beta^{0}_{5})$ can be chosen to generate "treatment effect patterns" by specifying $\psi^{0}(z)$ values at 
$z=0$, $z=k$, and $z=\zeta$ for $\zeta > k$.  For specified $\psi^{0}(0)$, $\psi^{0}(k)$, and $\psi^{0}(\zeta)$ we have 

\begin{eqnarray}
\beta^{0}_{1} &=& \psi^{0}(0), \cr
\beta^{0}_{3} &=& {(\psi^{0}(k) - \beta^{0}_{1}) \over k}, \cr
\beta^{0}_{5} &=& {(\psi^{0}(\zeta) - \beta^{0}_{1} - \beta^{0}_{3}\zeta) \over (\zeta -k)}.
\end{eqnarray}


The function *get_dgm_stratified* generates $\psi^{0}(z)$ according to desired "biomarker treatment effect patterns" as follows.

- Let $X$ and $Z$ denote the treatment and biomarker variables in the case-study dataset and for specified $k$, form the covariates $L:=(X,Z,ZX,(Z-k)I(Z>k),(Z-k)I(Z>k)X))$;
- Fit the Weibull model (recall on AFT scale) to get $\log(\hat\theta)$, $\hat\tau=1/\hat{\nu}$, and $\hat\gamma$ corresponding to $L$;
- $\hat\gamma$ is in terms of the AFT parameterization given by model (3)
- Next transform to the Weibull (Cox) log hazard-ratio parameterization (4): $\hat\beta = -\hat\gamma/\hat\tau$
- Set "true" parameters $\theta^{0}=\hat\theta$, and $\tau^{0}=\hat\tau$
- Initialize the "true" parameter $\beta^{0} = \hat\beta$ and re-define parameters 1, 3, and 5 in order to satisfy specified $\psi^{0}(0)$, $\psi^{0}(k)$, and $\psi^{0}(\zeta)$:
$\beta^{0}[1] = \psi^{0}(0)$, $\beta^{0}[3] = (\psi^{0}(k) - \beta^{0}[1])/k$, and $\beta^{0}[5] = (\psi^{0}(\zeta) - \beta^{0}[1] - \beta^{0}[3]\zeta)/(\zeta -k)$;
- Form corresponding $\gamma^{0}= -\beta^{0}\tau^{0}$
- For simulations we use the AFT parameterization (3) to generate $\log(T)$ outcomes according to $\log(T) = \log(\theta^{0}) + L'\gamma^{0} + \tau^{0}\epsilon$ where recall $\epsilon$ has the ``extreme value'' distribution.


## Example where treatment effects increase with increasing biomarker 

We assume that $z=0$ indicates biomarker negative values with $z>1$ indicating positive levels.  As an example, suppose the log(hazard ratio) at
$z=0$ is $\psi^{0}(0)=\log(3)$ and decreases linearly for $z \leq 5$ (with slope $\beta_{3}^{0}$) such that at $z=5$, $\psi^{0}(5)=\log(1.25)$ and 
for $z>5$ decreases linearly (with a change in slope and intercept) such that at $z=10$, $\psi^{0}(10) = \log(0.5)$.  In the following we will describe summary measures of the treatment effects as a function of increasing [or decreasing] values of the biomarker.  In this example treatment is detrimental 
for lower values of the biomarker with treatment effects increasing fairly quickly as the biomarker increases with an overall ``average hazard ratio'' (AHR) of $\approx 0.74$ (see Working Example below).  


## Including prognostic factors $W$

We extend the model to include a baseline prognostic factor $W$ within $L$ (effect parameter $\beta^{0}_{w}$) where

\begin{equation}
l_{x,z,w} = \beta^{0}_{1}x + \beta^{0}_{2}z + \beta^{0}_{3}zx + \beta^{0}_{4}(z-k)I(z>k) + 
 \beta^{0}_{5}(z-k)I(z>k)x + \beta^{0}_{w}w.
\end{equation}


Note that defining $m(1,z,w)$ [$m(0,z,w)$)] as the conditional expected value of $\log(T)$ with treatment set to experimental [control] given biomarker level $Z=z$ and $W=w$ 

$$\psi^{0}(z,w) = \{m(0,z,w)-m(1,z,w)\}/\tau$$
\noindent
which corresponds to the difference in the (true) means of the potential outcomes under experimental and control conditions ($\log(T[0,z,w]$ versus $\log(T[1,z,w]$, say) 


# Specifying biomarker treatment effects

## Average hazard ratios (AHRs)

We define the *biomarker average hazard ratio (AHR)* as the expected value of $\psi^{0}(\cdot)$ across "increasing biomarker" sub-populations.  For example, $\hbox{AHR}(2^{+})$ represents the AHR for subjects with biomarker values $\geq 2$ via  


\begin{equation}
\tag{6}
\hbox{AHR}(z^{+}):= \exp\left\{E_{Z \geq z} \psi^{0}(Z) \right\};
\end{equation}

With the opposite direction -- across "decreasing biomarker" sub-populations

\begin{equation}
\tag{7}
\hbox{AHR}(z^{-}):= \exp\left\{E_{Z \leq z} \psi^{0}(Z) \right\}.
\end{equation}

Here the expectations are over the distribution of $Z$, however if there is a prognostic factor $W$ then the expectations are over $(Z,W)$.  In our calculations we calculate empirical averages.


## Controlled direct-effect (CDE) versions (@ACR_2015)

Averaging across hazard ratios:  Define the hazard (omitting baseline hazard) $\theta^{x}(z,w) = \exp(l(x,z,w))$ setting treatment to $X=x$ given $Z=z$ and $W=w$.  Aalen et al. (@ACR_2015) define the controlled direct-effect (CDE) as the ratio of the expected hazards.  Here we consider the above cumulative versions (omitting possible dependence on $W$).   Let $\bar\theta^{x}(z+) = E_{Z \geq z}\theta^{x}(Z)$ for $x=0,1$, and define

$$\hbox{CDE}(z^{+}):= \bar\theta^{1}(z+)/\bar\theta^{0}(z+), \quad \hbox{and} \quad \hbox{CDE}(z^{-}):= \bar\theta^{1}(z-)/\bar\theta^{0}(z-)$$





