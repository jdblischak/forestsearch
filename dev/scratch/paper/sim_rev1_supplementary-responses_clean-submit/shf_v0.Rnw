\Sexpr{set_parent("supplementary_SIM-Rev1_v0.Rnw")}


<<echo=FALSE>>=
opts_chunk$set (warning = FALSE, message = FALSE, tidy=TRUE, echo=FALSE, dev = 'pdf')
options(warn=-1)
@
  
<<echo=FALSE>>=
rm(list=ls())
# For KM plots
codepath<-c("../../R/")
source(paste0(codepath,"source_forestsearch_v0.R"))
source_fs_functions(file_loc=codepath)

library(survival)
library(speff2trial)

load(file="../applications/systolic-heart-failure/output/shf_benefit_nolasso_v0.Rdata")
# Analysis dataset
df <- fs.est$df.est

df0.fs<-subset(fs.est$df.pred,treat.recommend==0)
df1.fs<-subset(fs.est$df.pred,treat.recommend==1)

outcome.name<-c("time_months")
event.name<-c("event")
id.name<-c("id")
treat.name<-c("treat")

# ITT analysis (Reverse treatment roles to original)
cox_itt<-summary(coxph(Surv(time_months,event)~treat,data=fs.est$df.pred))$conf.int
# ITT estimates
resITT<-c(round(cox_itt[c(1,3,4)],2),nrow(fs.est$df.pred))

df0.fs$treat2 <- 1-df0.fs$treat
df1.fs$treat2 <- 1-df1.fs$treat

df.analysis <- fs.est$df.pred

# plot_twosample(df=df.analysis,tte.name=outcome.name,treat.name="aspirin",event.name=event.name,
# col.treat="blue",col.control="darkgrey",choose_ylim=TRUE,               
# ylab="Survival Probability", xlab="Months", show.Y.axis=TRUE,               
# byrisk=12,show.med=FALSE,legend.cex=0.5,risk.cex=0.5,censor.cex=0.7,cox.cex=0.55,cex_Yaxis=0.65)

suppressMessages(library(kableExtra,quietly=TRUE))
suppressMessages(library(knitr,quietly=TRUE))
suppressMessages(library(randomForest,quietly=TRUE))
suppressMessages(library(survival,quietly=TRUE))
suppressMessages(library(grf,quietly=TRUE))
suppressMessages(library(policytree,quietly=TRUE))
suppressMessages(library(DiagrammeR,quietly=TRUE))
suppressMessages(library(data.table,quietly=TRUE))
suppressMessages(library(plyr,quietly=TRUE))
suppressMessages(library(dplyr,quietly=TRUE))
suppressMessages(library(glmnet,quietly=TRUE))
# smd and gtsummary are for baseline summary table
# omit if not desired (remove corresponding section below)
suppressMessages(library(smd,quietly=TRUE)) # required for gtsummary
suppressMessages(library(gtsummary,quietly=TRUE)) # only for baseline table summary
suppressMessages(library(future,quietly=TRUE))
library(stringr)



@
  
  
  
<<echo=FALSE,message=FALSE,warning=FALSE>>=
H_estimates <- round(fs_bc$H_estimates,2)
Hc_estimates <- round(fs_bc$Hc_estimates,2)

resQ <- unlist(H_estimates[,c("H0","H0_lower","H0_upper")])
resQB <- unlist(H_estimates[,c("H2","H2_lower","H2_upper")])
resQc <- unlist(Hc_estimates[,c("H0","H0_lower","H0_upper")])
resQcB <- unlist(Hc_estimates[,c("H2","H2_lower","H2_upper")])

L_factors <- fs.est$find.grps$L
max_count <- fs.est$find.grps$max_count
sg_loc <- fs.est$grp.consistency$sg.harm.id
SG_hrs <- fs.est$grp.consistency$out_hr$result
# Here, only 1 SG is returned (Need to check this manually)
# Here override sg_loc =1
sg_loc <- 1
p_consistency <- 100*round(SG_hrs$Pcons[sg_loc],3)

# N-fold SGs
SG_tab_Kfold <- summary_OOB$SG_tab_Kfold$res_out
nH_OOB <- SG_tab_Kfold[1,c("n")]
# Full analysis
SG_tab_full <- summary_OOB$SG_tab_original$res_out
nH_full <- SG_tab_full[1,c("n")]

H_nfold <- summary_OOB$SG_tab_Kfold$res_out[1,8]
Hc_nfold <- summary_OOB$SG_tab_Kfold$res_out[2,8]

# Number of SGs found in N-fold
nf_OOB <- round(fs_OOB$prop_SG_found*nrow(df)/100,0)
# Proportion found
pf_OOB <- fs_OOB$prop_SG_found

L1 <- 78*2

L2 <- 379*2

L3 <- 151*2

@
  
  
  
<<echo=FALSE,message=FALSE,warning=FALSE,eval=FALSE>>=
# To update, need to re-evaluate (set eval=TRUE)
library(latex2exp)

# Reverse treatment roles to original via treat2
df.analysis <- fs.est$df.pred

pdf(file = "plot_KM_shf.pdf", 
width = 10, height = 10) 

par(mfrow=c(1,2))

tte.name<-c("time_months")
event.name<-c("event")
# Revert back to original treatment 1-vs-3 comparison
treat.name<-c("treat2")
byrisk <- 12

risk.cex<-0.95
legend.cex<-0.90

# Add all events
evs<-sort(unique(df.analysis$time_months[which(df.analysis$event==1)]))
tpoints.add<-c(-1,evs,max(df.analysis$time_months))

# Legend labels for Q
# Q is "especially strong"

Hla<-sprintf(r'($\hat{Q}, %s$)',c("Treatment"))
Hlb<-sprintf(r'($\hat{Q}, %s$)',c("Control"))
Hl<-c(Hla,Hlb)

Hcla<-sprintf(r'($\hat{Q^{c}}, %s$)',c("Treatment"))
Hclb<-sprintf(r'($\hat{Q^{c}}, %s$)',c("Control"))
Hcl<-c(Hcla,Hclb)

dfp<-df0.fs

kmH.fit<-KM.plot.2sample.weighted(Y=dfp[,c(tte.name)],E=dfp[,c(event.name)],Treat=dfp[,c(treat.name)],
risk.set=TRUE,by.risk=byrisk,tpoints.add=tpoints.add,risk.cex=risk.cex,
choose_ylim=TRUE,
stop.onerror=TRUE, check.KM=TRUE,
Xlab="Months",Ylab="Survival",details=FALSE,
show.ticks=TRUE,
col.1="black", col.2="blue",
ltys=c(1,2),lwds=c(3,3),
plotit=TRUE,
show.logrank=FALSE,show.med=FALSE,show.cox=FALSE)
title(sub="(a) Hgb > 13.84 and Resting.hr <= 86")

legend("topright",
legend=TeX(Hl),col=c("black","blue"),lwd=c(3,3),lty=c(1,2),bty="n",cex=legend.cex)


dfp<-df1.fs

kmH.fit<-KM.plot.2sample.weighted(Y=dfp[,c(tte.name)],E=dfp[,c(event.name)],Treat=dfp[,c(treat.name)],
risk.set=TRUE,by.risk=byrisk,tpoints.add=tpoints.add,risk.cex=risk.cex,
show.Y.axis=FALSE,
choose_ylim=TRUE,
stop.onerror=TRUE, check.KM=TRUE,
Xlab="Months",Ylab="",details=FALSE,
show.ticks=TRUE,
col.1="black", col.2="blue",
ltys=c(1,2),lwds=c(3,3),
plotit=TRUE,
show.logrank=FALSE,show.med=FALSE,show.cox=FALSE)
title(sub="(b) Hgb <= 13.84 or Resting.hr > 86")

legend("topright",
legend=TeX(Hcl),col=c("black","blue"),lwd=c(3,3),lty=c(1,2),bty="n",cex=legend.cex)

dev.off()

@



\subsection*{S2.5 Systolic heart failure data analysis}

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth, height=3.25in]{plot_KM_shf.pdf}
\end{center}
\caption{Systolic heart failure data analysis application of Forest Search. Kaplan-Meier (K-M) curves (un-adjusted for the estimation of subgroups): (a) Forest Search $\widehat{Q}$ subgroup treatment estimates;
(b) Forest Search $\widehat{Q}^{c}$ subgroup treatment estimates.}
\label{fig:KM_shf}
\end{figure}

Analysis evaluating the use of aspirin in the systolic heart failure data (\citealp{Hsich_2011}) available in the \verb+randomForestSRC+ (\citealp{Ishwaran_2008}) package ($N=2,231$ subjects, $p = 38$ baseline covariates, and $K=78$). We also induce computational challenges by adding $100$ noise factors and discuss mitigation approaches when the resulting number of subgroup candidate factors is large, $K=379$.

We note that this is an observational analysis as the use of aspirin was not randomized.  As noted in the discussion section, the FS approach can be extended to adjust for covariates via (stabilized) propensity score weighting \citealp{Cole_2004}, however this is beyond the scope of the current paper 
and will be a direction for further research.   The analyses presented here are for illustration purposes to evaluate the feasibility of including a large number of baseline factors.

The $p=38$ baseline covariates are summarized in Table \ref{tab:bl_exposure}. Figure \ref{fig:KM_shf} displays the Kaplan-Meier curves for the estimated subgroups.

In this example we focus on the feasibility with respect to computational timings (details can be found in the R markdown files at the github site referenced in the introduction).   Of the 38 baseline covariates, 13 were continuous and 25 were categorical factors.   The number of candidate subgroup (binary) factors was $K=78$ ($L=156$ single factor subgroups) with number of all-possible 2 factor combinations 
$L(L-1)/2+L = \Sexpr{prettyNum(L1*(L1-1)/2+L1, big.mark=",")}$.  The computational timing on an Apple studio (M1 20 core with 69 GB) was approximately: $0.5$ minutes for the FS analysis; $120$ minutes for the $2000$ bootstraps; $\Sexpr{round(fs_OOB$timing_minutes,0)}$ minutes for the $N$-fold cross-validation; and $\Sexpr{round(tall.min-252.11,0)}$ minutes for the $200$ random 10-fold cross-validation analyses.  In total, the number of minutes was $\approx \Sexpr{round(tall.min,0)}$.


We also consider adding 100 random noise factors which resulted in $K=379$ factors ($L=\Sexpr{2*379}$) and $\Sexpr{prettyNum(L2*(L2-1)/2+L2, big.mark=",")}$ all-possible 2 factor combinations.   Now, to reduce the computational burden we utilize the variable importance (VI) measure from the 
\verb+causal_survival_forest+ function in the \R{} \verb+grf+ package (\citealp{grfR,SAW_2023}).  It is not clear what VI values constitute an ``important factor''; here we consider excluding factors with VI measures which are less than $10\%$ of the factor with the largest VI measure.  That is, 
if $\hbox{vimax}$ denotes the largest VI measure, then only factors with VI measure $\geq \hbox{vimax}/10$ are included.   This resulted in $K=151$ and  $\Sexpr{prettyNum(L3*(L3-1)/2+L3, big.mark=",")}$ all-possible 2 factor combinations.  In our simulations we found $400$ bootstraps to have good performance for 
bias-reduction and variance estimation.  We therefore applied $B=500$ bootstraps where the above algorithm (with VI measure criterion) is incorporated in the procedure.   Here the timing for the FS analysis was 1.4 minutes and 56 minutes for the bootstrapping.



<<echo=FALSE,message=FALSE,warning=FALSE>>=
data(peakVO2, package = "randomForestSRC")

df.analysis <- peakVO2

df.analysis<-within(df.analysis,{
treat <- aspirin
})

dfbl_exp <- df.analysis 

as_kable_tbl <-
  tbl_summary(
  dfbl_exp,
  by = treat, # split table by group
  missing = "no" # don't list missing data separately
  ) %>%
  add_n() %>% # add column with total number of non-missing observations
  add_difference(pvalue_fun=function(x){round(x,3)})%>%
  add_stat_label

as_kable_tbl <- 
  as_kable_tbl %>% 
  as_kable_extra(
    booktabs = TRUE,
    longtable = FALSE,
    escape=TRUE,
    caption="\\label{tab:bl_exposure} Summary of events and baseline factors by aspirin use (1=yes, 0=no)",
    linesep = "") %>%
    kableExtra::kable_styling(bootstrap_options=c("striped","hover"), full_width=F, position="center",font_size=10,latex_options=c("scale_down","hold_position")
  )

as_kable_tbl

@








 
  
  