---
title: "Simulation Studies for Evaluating ForestSearch Performance"
subtitle: "Operating Characteristics and Power Analysis"
author: "ForestSearch Package"
date: today
format:
  html:
    self-contained: true
    toc: true
    toc-depth: 3
    code-fold: false
    code-summary: "Show code"
    number-sections: true
    theme: cosmo
    highlight-style: github
execute:
  warning: false
  message: false
  eval: true
vignette: >
  %\VignetteIndexEntry{Simulation Studies for Evaluating ForestSearch Performance}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 9,
  fig.height = 6
)
```

# Introduction

This vignette demonstrates how to conduct simulation studies to evaluate the
performance of ForestSearch for identifying subgroups with differential treatment
effects. The simulation framework allows you to:
 
- Generate synthetic clinical trial data with known treatment effect heterogeneity
- Evaluate subgroup identification rates (power)
- Assess classification accuracy (sensitivity, specificity, PPV, NPV)
- Compare different analysis methods (ForestSearch, GRF)
- Estimate Type I error under null hypothesis

## Simulation Framework Overview
 
The simulation workflow consists of four main steps:

```{mermaid}
flowchart LR
    A[Create DGM] --> B[Simulate Trials]
    B --> C[Run Analyses]
    C --> D[Summarize Results]
```

1. **Create DGM**: Define a data generating mechanism with specified treatment effects
2. **Simulate Trials**: Generate multiple simulated datasets
3. **Run Analyses**: Apply ForestSearch (and optionally GRF) to each dataset
4. **Summarize Results**: Aggregate operating characteristics across simulations

# Setup

```{r load-packages}
# Core packages
library(forestsearch)
library(weightedsurv)

library(data.table)
library(survival)
library(ggplot2)
library(gt)

# Parallel processing
library(foreach)
library(doFuture)
library(future)

# Source simulation functions (if not yet in package)
# source("sim_aft_gbsg_refactored.R")
# source("oc_analyses_refactored.R")
```

# Creating a Data Generating Mechanism

The simulation framework uses the German Breast Cancer Study Group (GBSG) dataset
as a template for realistic covariate distributions and censoring patterns.

## Understanding the DGM

The `create_gbsg_dgm()` function creates a data generating mechanism (DGM) based
on an Accelerated Failure Time (AFT) model with Weibull distribution. Key features:

- **Covariates**: Age, estrogen receptor, menopausal status, progesterone receptor, nodes
- **Treatment effect heterogeneity**: Specified via interaction terms
- **Subgroup definition**: H = {low estrogen receptor AND premenopausal}
- **Censoring**: Weibull or uniform censoring model

## Alternative Hypothesis (Heterogeneous Treatment Effect)

Under the alternative hypothesis, we create a DGM where the treatment effect
varies across patient subgroups:

```{r create-dgm-alt}
# Create DGM with heterogeneous treatment effect
# HR in harm subgroup (H) will be > 1 (treatment harmful)
# HR in complement (H^c) will be < 1 (treatment beneficial)

dgm_alt <- create_gbsg_dgm(
  model = "alt",
  k_treat = 1.0,
  k_inter = 2.0,      # Interaction effect multiplier
  k_z3 = 1.0,
  z1_quantile = 0.25, # ER threshold at 25th percentile
  n_super = 5000,
  cens_type = "weibull",
  seed = 8316951,
  verbose = TRUE
)

# Examine the DGM
print(dgm_alt)
```

## Calibrating for a Target Hazard Ratio

Often, you want to specify the exact hazard ratio in the harm subgroup. Use
`calibrate_k_inter()` to find the interaction parameter that achieves this:

```{r calibrate-k-inter}
# Find k_inter for HR = 1.5 in harm subgroup
k_inter_target <- calibrate_k_inter(
  target_hr_harm = 2.0,
  model = "alt",
  k_treat = 1.0,
  cens_type = "weibull",
  verbose = TRUE
)

# Create DGM with calibrated k_inter
dgm_calibrated <- create_gbsg_dgm(
  model = "alt",
  k_treat = 1.0,
  k_inter = k_inter_target,
  verbose = TRUE
)

cat("\nVerification:\n")
cat("Achieved HR(H):", round(dgm_calibrated$hr_H_true, 3), "\n")
cat("HR(H^c):", round(dgm_calibrated$hr_Hc_true, 3), "\n")
cat("Overall HR:", round(dgm_calibrated$hr_causal, 3), "\n")
```

## Null Hypothesis (Uniform Treatment Effect)

For Type I error evaluation, create a DGM with uniform treatment effect:

```{r create-dgm-null}
# Create null DGM (no treatment effect heterogeneity)
dgm_null <- create_gbsg_dgm(
  model = "null",
  k_treat = 1.0,
  verbose = TRUE
)

cat("\nNull hypothesis HRs:\n")
cat("Overall HR:", round(dgm_null$hr_causal, 3), "\n")
cat("HR(H^c):", round(dgm_null$hr_Hc_true, 3), "\n")
```

# Simulating Trial Data

## Single Trial Simulation

Use `simulate_from_gbsg_dgm()` to generate a single simulated trial:

```{r simulate-single}
# Simulate a single trial
sim_data <- simulate_from_gbsg_dgm(
  dgm = dgm_calibrated,
  n = 700,
  rand_ratio = 1,        # 1:1 randomization
  sim_id = 1,
  max_follow = 84,       # 84 months administrative censoring
  muC_adj = log(1.5)            # No censoring adjustment
)

# Examine the data
cat("Simulated trial:\n")
cat("  N =", nrow(sim_data), "\n")
cat("  Events =", sum(sim_data$event.sim), 
    "(", round(100 * mean(sim_data$event.sim), 1), "%)\n")
cat("  Harm subgroup size =", sum(sim_data$flag.harm),
    "(", round(100 * mean(sim_data$flag.harm), 1), "%)\n")

# Quick survival analysis
fit_itt <- coxph(Surv(y.sim, event.sim) ~ treat, data = sim_data)
cat("  Estimated ITT HR =", round(exp(coef(fit_itt)), 3), "\n")
```

## Examining Covariate Structure

```{r, fig.width = 7, fig.height = 5}

dfcount <- df_counting(
  df = sim_data,
  by.risk = 6,
  tte.name = "y.sim", 
  event.name = "event.sim", 
  treat.name = "treat"
)
plot_weighted_km(dfcount, conf.int = TRUE, show.logrank = TRUE, ymax = 1.05, xmed.fraction = 0.775, ymed.offset = 0.125)

```





```{r}
create_summary_table(data = sim_data, treat_var = "treat", 
                     table_title = "Characteristics by Treatment Arm",
                                      vars_continuous=c("z1","z2","size","z3","z4","z5"),
                                      vars_categorical=c("flag.harm","grade3"),
                                      font_size = 12)
```


# Running Simulation Studies

## Setting Up Parallel Processing

For efficient simulation studies, use parallel processing:

```{r setup-parallel}
# Configure parallel backend
n_workers <- min(parallel::detectCores() - 1, 120)

plan(multisession, workers = n_workers)
registerDoFuture()

cat("Using", n_workers, "parallel workers\n")
```

## Define Simulation Parameters

```{r define-params}
# Simulation settings
sim_config_alt <- list(
  n_sims = 1000,          # Number of simulations (use 500-1000 for final)
  n_sample = 700,        # Sample size per trial
  max_follow = 84,       # Maximum follow-up (months)
  seed_base = 8316951,
  muC_adj = log(1.5)
)

sim_config_null <- list(
  n_sims = 1000,          # Number of simulations (use 500-1000 for final)
  n_sample = 700,        # Sample size per trial
  max_follow = 84,       # Maximum follow-up (months)
  seed_base = 8316951,
  muC_adj = log(1.5)
)


# ForestSearch parameters
fs_params <- list(
  outcome.name = "y.sim",
  event.name = "event.sim",
  treat.name = "treat",
  id.name = "id",
  hr.threshold = 1.0,
  hr.consistency = 0.90,
  pconsistency.threshold = 0.90,
  use_grf = TRUE,
  use_lasso = FALSE,
  fs.splits = 400,
  n.min = 60,
  maxk = 2
)

# Confounders for analysis
confounders_base <- c("z1", "z2", "z3", "z4", "z5", "size", "grade3")
```

## Running the Simulation Loop

```{r run-simulation}
# Track timing
t_start <- Sys.time()

sim_config <- sim_config_alt

# Run simulations in parallel
results_alt <- foreach(
  sim = 1:sim_config$n_sims,
  .combine = rbind,
  .errorhandling = "remove",
  .options.future = list(
    packages = c("forestsearch", "survival", "data.table"),
    seed = TRUE
  )
) %dofuture% {
# Run single simulation analysis
run_simulation_analysis(
    sim_id = sim,
    dgm = dgm_calibrated,
    n_sample = sim_config$n_sample,
    max_follow = sim_config$max_follow,
    muC_adj = sim_config$muC_adj,
    confounders_base = confounders_base,
    n_add_noise = 0,
    run_fs = TRUE,
    run_fs_grf = FALSE,
    run_grf = TRUE,      # Set TRUE if grf_subg_harm_survival available
    fs_params = fs_params,
    n_sims_total = sim_config$n_sims,
    seed_base = sim_config$seed_base,
    verbose = TRUE,
    verbose_n = 1
  )
}

# Calculate runtime
t_end <- Sys.time()
runtime_mins <- as.numeric(difftime(t_end, t_start, units = "mins"))

cat("\n=== Simulation Complete ===\n")
cat("Total simulations:", sim_config$n_sims, "\n")
cat("Successful:", nrow(results_alt) / length(unique(results_alt$analysis)), "\n")
cat("Runtime:", round(runtime_mins, 2), "minutes\n")
cat("Per simulation:", round(runtime_mins / sim_config$n_sims * 60, 1), "seconds\n")
```

## Running Null Hypothesis Simulations (Type I Error)

```{r run-null-simulation}

sim_config <- sim_config_null

# Track timing
t_start <- Sys.time()

# Run null hypothesis simulations
results_null <- foreach(
  sim = 1:sim_config$n_sims,
  .combine = rbind,
  .errorhandling = "remove",
  .options.future = list(
    packages = c("forestsearch", "survival", "data.table"),
    seed = TRUE
  )
) %dofuture% {
  
  run_simulation_analysis(
    sim_id = sim,
    dgm = dgm_null,
    n_sample = sim_config$n_sample,
    max_follow = sim_config$max_follow,
    muC_adj = sim_config$muC_adj,
    confounders_base = confounders_base,
    run_fs = TRUE,
    run_fs_grf = FALSE,
    run_grf = TRUE,
    fs_params = fs_params,
    verbose = TRUE,
    verbose_n = 1
  )
}

cat("Null simulations completed:", 
    nrow(results_null) / length(unique(results_null$analysis)), "\n")

# Calculate runtime
t_end <- Sys.time()
runtime_mins <- as.numeric(difftime(t_end, t_start, units = "mins"))

cat("\n=== Simulation Complete ===\n")
cat("Total simulations:", sim_config$n_sims, "\n")
cat("Successful:", nrow(results_null) / length(unique(results_null$analysis)), "\n")
cat("Runtime:", round(runtime_mins, 2), "minutes\n")
cat("Per simulation:", round(runtime_mins / sim_config$n_sims * 60, 1), "seconds\n")

```

# Analyzing Simulation Results

## Summary Statistics

```{r summarize-results}
# Summarize alternative hypothesis results
summary_alt <- summarize_simulation_results(
  results = results_alt,
  digits = 2,
  digits_hr = 3
)

# Summarize null hypothesis results  
summary_null <- summarize_simulation_results(
  results = results_null,
  digits = 2,
  digits_hr = 3
)

# Display summaries
cat("=== Alternative Hypothesis (Power) ===\n")
print(summary_alt)


cat("\n=== Null Hypothesis (Type I Error) ===\n")
print(summary_null)
```

## Formatted Tables with gt

```{r gt-tables}
# Create formatted table for alternative hypothesis

tbl_alt <- format_oc_results(
  results = results_alt,
  digits = 2,
  digits_hr = 3
)

tbl_alt
```

```{r gt-table-null}
# Create formatted table for null hypothesis
tbl_null <- format_oc_results(
  results = results_null,
  digits = 2,
  digits_hr =3
)

tbl_null
```

## Key Operating Characteristics

```{r key-metrics}
# Extract key metrics for comparison
methods <- unique(results_alt$analysis)

metrics_comparison <- data.frame(
  Method = methods,
  Power = sapply(methods, function(m) {
    mean(results_alt[results_alt$analysis == m, ]$any.H)
  }),
  TypeI = sapply(methods, function(m) {
    mean(results_null[results_null$analysis == m, ]$any.H)
  }),
  Sensitivity = sapply(methods, function(m) {
    mean(results_alt[results_alt$analysis == m, ]$sensitivity, na.rm = TRUE)
  }),
  PPV = sapply(methods, function(m) {
    mean(results_alt[results_alt$analysis == m, ]$ppv, na.rm = TRUE)
  })
)

metrics_comparison |>
  gt() |>
  tab_header(
    title = "Method Comparison",
    subtitle = sprintf("N = %d, n = %d per trial", 
                       sim_config$n_sims, sim_config$n_sample)
  ) |>
  fmt_percent(columns = c(Power, TypeI, Sensitivity, PPV), decimals = 1) |>
  cols_label(
    Method = "Analysis Method",
    Power = "Power",
    TypeI = "Type I Error",
    Sensitivity = "Sensitivity",
    PPV = "PPV"
  )
```

# Visualizing Results

## Subgroup Detection Rates

```{r plot-detection, fig.height=5, fig.width=8}
# Detection rates by method
detection_data <- results_alt[, .(
  detection_rate = mean(any.H),
  se = sqrt(mean(any.H) * (1 - mean(any.H)) / .N)
), by = analysis]

ggplot(detection_data, aes(x = analysis, y = detection_rate, fill = analysis)) +
  geom_col(width = 0.6) +
  geom_errorbar(
    aes(ymin = detection_rate - 1.96 * se, 
        ymax = detection_rate + 1.96 * se),
    width = 0.2
  ) +
  geom_hline(yintercept = 0.80, linetype = "dashed", color = "red", alpha = 0.7) +
  annotate("text", x = 0.6, y = 0.82, label = "80% Power", hjust = 0, size = 3) +
  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
  labs(
    x = "Analysis Method",
    y = "Subgroup Detection Rate",
    title = "Power to Detect Harm Subgroup",
    subtitle = sprintf("HR(H) = %.2f, n = %d, %d simulations",
                       dgm_calibrated$hr_H_true, 
                       sim_config$n_sample,
                       sim_config$n_sims)
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## Hazard Ratio Distributions

```{r plot-hr-distributions, fig.height=6, fig.width=10}
# Prepare data for plotting
hr_long <- melt(
  results_alt,
  id.vars = c("sim", "analysis", "any.H"),
  measure.vars = c("hr.itt", "hr.H.hat", "hr.Hc.hat"),
  variable.name = "estimand",
  value.name = "hr"
)

hr_long$estimand <- factor(hr_long$estimand, 
  levels = c("hr.itt", "hr.H.hat", "hr.Hc.hat"),
  labels = c("ITT", "Harm Subgroup (H)", "Complement (H^c)")
)

# True values for reference
true_hrs <- data.frame(
  estimand = c("ITT", "Harm Subgroup (H)", "Complement (H^c)"),
  true_hr = c(dgm_calibrated$hr_causal, 
              dgm_calibrated$hr_H_true, 
              dgm_calibrated$hr_Hc_true)
)

ggplot(hr_long[!is.na(hr_long$hr), ], 
       aes(x = estimand, y = hr, fill = analysis)) +
  geom_violin(position = position_dodge(0.8), alpha = 0.7) +
  geom_boxplot(position = position_dodge(0.8), width = 0.15, 
               outlier.size = 0.5, alpha = 0.9) +
  geom_hline(data = true_hrs, aes(yintercept = true_hr), 
             linetype = "dashed", color = "red") +
  geom_hline(yintercept = 1, linetype = "solid", color = "gray50", alpha = 0.5) +
  scale_y_log10() +
  labs(
    x = "Estimand",
    y = "Hazard Ratio (log scale)",
    fill = "Method",
    title = "Distribution of Hazard Ratio Estimates",
    subtitle = "Dashed red lines indicate true values"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 15, hjust = 1))
```

## Classification Performance

```{r plot-classification, fig.height=5, fig.width=9}
# Classification metrics
class_metrics <- results_alt[any.H == 1, .(
  Sensitivity = mean(sensitivity, na.rm = TRUE),
  Specificity = mean(specificity, na.rm = TRUE),
  PPV = mean(ppv, na.rm = TRUE),
  NPV = mean(npv, na.rm = TRUE)
), by = analysis]

class_long <- melt(class_metrics, id.vars = "analysis", 
                   variable.name = "metric", value.name = "value")

ggplot(class_long, aes(x = metric, y = value, fill = analysis)) +
  geom_col(position = position_dodge(0.8), width = 0.7) +
  geom_hline(yintercept = 0.8, linetype = "dashed", alpha = 0.5) +
  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
  labs(
    x = "Classification Metric",
    y = "Value",
    fill = "Method",
    title = "Classification Performance (When Subgroup Found)",
    subtitle = "Sensitivity = P(in Ĥ | in H), PPV = P(in H | in Ĥ)"
  ) +
  theme_minimal()
```

## Subgroup Size Distribution

```{r plot-sg-size, fig.height=5, fig.width=8}
# Size distribution when subgroup found
size_data <- results_alt[any.H == 1]

ggplot(size_data, aes(x = size.H, fill = analysis)) +
  geom_histogram(position = "identity", alpha = 0.6, bins = 30) +
  geom_vline(xintercept = sim_config$n_sample * mean(dgm_calibrated$df_super_rand$flag.harm),
             linetype = "dashed", color = "red") +
  facet_wrap(~ analysis, ncol = 1) +
  labs(
    x = "Estimated Subgroup Size",
    y = "Count",
    title = "Distribution of Estimated Subgroup Sizes",
    subtitle = "Dashed line = expected true subgroup size"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

# Theoretical Subgroup Detection Rate Approximation

The function `compute_detection_probability()` provides an analytical approximation
based on asymptotic normal theory:

```{r theoretical-detection-rates}

#| label: theoretical-power
#| fig-width: 8
#| fig-height: 6

# # Quick check using older code
# hr1 <- 1.25
# hr2 <- 1.0
# library(cubature)
# # Mild censoring
# n.sg <- 60
# pC <- 0.20
# hrH.plims<-seq(0.5,3.0,length=60)
# hrH.plims <- sort(c(hrH.plims,0.5,0.7,0.75,0.80))
# 
# pAnyH.approx <-rep(NA,length(hrH.plims))
# for(hh in 1:length(hrH.plims)){
#   hrH.plim <- hrH.plims[hh]
#   pAnyH.approx[hh] <- compute_detection_probability(
#  theta = hrH.plim,
#   n_sg = n.sg,
#   prop_cens = pC,
#   hr_threshold = hr1,
#   hr_consistency = hr2,
#   method = "cubature"
# )
# }
# plot(hrH.plims,pAnyH.approx,xlab="Hazard ratio for subgroup H",ylab="Probability of any H found",type="l",lty=1,lwd=2.0,ylim=c(0,1))
# abline(h=0.10,lwd=0.5,col="red")
# Looks good!

# =============================================================================
# Theoretical Detection Probability Analysis
# =============================================================================

# Calculate expected subgroup characteristics
n_sg_expected <- sim_config_alt$n_sample * mean(dgm_calibrated$df_super_rand$flag.harm)
prop_cens <- mean(results_alt$p.cens)  # Censoring proportion

cat("=== Subgroup Characteristics ===\n")
cat("Expected subgroup size (n_sg):", round(n_sg_expected), "\n")
cat("Censoring proportion:", round(prop_cens, 3), "\n")
cat("True HR in H:", round(dgm_calibrated$hr_H_true, 3), "\n")
cat("HR threshold:", fs_params$hr.threshold, "\n")

# -----------------------------------------------------------------------------
# Single-Point Detection Probability
# -----------------------------------------------------------------------------

# True H is dgm_calibrated$hr_H_true
# However we want at plim of observed estimate
#plim_hr_hatH <- c(summary_alt[c("hat(hat[H])"),1])

dgm_calibrated$hr_H_true

# Compute detection probability at the true HR
prob_detect <- compute_detection_probability(
 theta = dgm_calibrated$hr_H_true,
  n_sg = round(n_sg_expected),
  prop_cens = prop_cens,
  hr_threshold = fs_params$hr.threshold,
  hr_consistency = fs_params$hr.consistency,
  method = "cubature"
)

# Compare theoretical to empirical (alternative)
cat("\n=== Detection Probability Comparison ===\n")
cat("Theoretical FS (asymptotic):", round(prob_detect, 3), "\n")

cat("Empirical FS:", round(mean(results_alt[analysis == "FS"]$any.H), 3), "\n")
cat("Empirical FSlg:", round(mean(results_alt[analysis == "FSlg"]$any.H), 3), "\n")
if ("GRF" %in% results_alt$analysis) {
  cat("Empirical GRF:", round(mean(results_alt[analysis == "GRF"]$any.H), 3), "\n")
}

# Null 

#plim_hr_itt <- c(summary_alt[c("hat(ITT)all"),1])

# Calculate at min SG size
# Compute detection probability at the true HR
prob_detect_null <- compute_detection_probability(
 theta = dgm_null$hr_causal,
  n_sg = fs_params$n.min,
  prop_cens = prop_cens,
  hr_threshold = fs_params$hr.threshold,
  hr_consistency = fs_params$hr.consistency,
  method = "cubature"
)


# Compare theoretical to empirical (alternative)
cat("\n=== Detection Probability Comparison ===\n")
cat("Under the null calculate at min SG size:", fs_params$n.min,"\n")
cat("Theoretical FS at min(SG) (asymptotic):", round(prob_detect_null, 6), "\n")

cat("Empirical FS:", round(mean(results_null[analysis == "FS"]$any.H), 6), "\n")
cat("Empirical FSlg:", round(mean(results_null[analysis == "FSlg"]$any.H), 6), "\n")
if ("GRF" %in% results_null$analysis) {
  cat("Empirical GRF:", round(mean(results_null[analysis == "GRF"]$any.H), 6), "\n")
}



# -----------------------------------------------------------------------------
# Generate Full Detection Curve
# -----------------------------------------------------------------------------

# Generate detection probability curve across HR values
detection_curve <- generate_detection_curve(
  theta_range = c(0.5, 3.0),
  n_points = 50,
  n_sg = round(n_sg_expected),
  prop_cens = prop_cens,
  hr_threshold = fs_params$hr.threshold,
  hr_consistency = fs_params$hr.consistency,
  include_reference = TRUE,
  verbose = FALSE
)

# -----------------------------------------------------------------------------
# Visualization
# -----------------------------------------------------------------------------

# Plot detection curve with empirical overlay
plot_detection_curve(
  detection_curve,
  add_reference_lines = TRUE,
  add_threshold_line = TRUE,
  title = sprintf(
    "Detection Probability Curve (n=%d, cens=%.0f%%, threshold=%.2f)",
    round(n_sg_expected), 100 * prop_cens, fs_params$hr.threshold
  )
)

# Add empirical results as points
empirical_rates <- c(
  FS = mean(results_alt[analysis == "FS"]$any.H),
  FSlg = mean(results_alt[analysis == "FSlg"]$any.H)
)
if ("GRF" %in% results_alt$analysis) {
  empirical_rates["GRF"] <- mean(results_alt[analysis == "GRF"]$any.H)
}

# Mark the true HR and empirical detection rates
points(
  x = rep(dgm_calibrated$hr_H_true, length(empirical_rates)),
  y = empirical_rates,
  pch = c(16, 17, 18)[1:length(empirical_rates)],
  col = c("blue", "darkgreen", "purple")[1:length(empirical_rates)],
  cex = 1.5
)

# Add vertical line at true HR
abline(v = dgm_calibrated$hr_H_true, lty = 2, col = "blue", lwd = 1)

# Legend for empirical points
legend(
  "topleft",
  legend = c(
    sprintf("H true = %.2f", dgm_calibrated$hr_H_true),
    paste(names(empirical_rates), "=", round(empirical_rates, 3))
  ),
  pch = c(NA, 16, 17, 18)[1:(length(empirical_rates) + 1)],
  lty = c(2, rep(NA, length(empirical_rates))),
  col = c("blue", "blue", "darkgreen", "purple")[1:(length(empirical_rates) + 1)],
  cex = 0.8,
  bty = "n"
)

# -----------------------------------------------------------------------------
# Compare Across Sample Sizes (Optional)
# -----------------------------------------------------------------------------

# Show how detection probability changes with sample size
cat("\n=== Detection Probability by Sample Size ===\n")
cat(sprintf("At true HR = %.2f:\n", dgm_calibrated$hr_H_true))

for (n_mult in c(0.5, 1.0, 1.5, 2.0)) {
  n_test <- round(n_sg_expected * n_mult)
  prob_test <- compute_detection_probability(
    theta = dgm_calibrated$hr_H_true,
    n_sg = n_test,
    prop_cens = prop_cens,
    hr_threshold = fs_params$hr.threshold,
    hr_consistency = fs_params$hr.consistency
  )
  cat(sprintf("  n_sg = %3d (%.1fx): P(detect) = %.3f\n", 
              n_test, n_mult, prob_test))
}

```

# Advanced Topics

## Adding Noise Variables

Test ForestSearch robustness by including irrelevant noise variables:

```{r noise-vars, eval=FALSE}
# Run simulations with noise variables
results_noise <- foreach(
  sim = 1:sim_config$n_sims,
  .combine = rbind,
  .errorhandling = "remove",
  .options.future = list(
    packages = c("forestsearch", "survival", "data.table"),
    seed = TRUE
  )
) %dofuture% {
  
  run_simulation_analysis(
    sim_id = sim,
    dgm = dgm_calibrated,
    n_sample = sim_config$n_sample,
    confounders_base = confounders_base,
    n_add_noise = 10,  # Add 10 noise variables
    run_fs = TRUE,
    fs_params = fs_params,
    verbose = FALSE
  )
}

# Compare detection rates
cat("Without noise:", round(mean(results_alt$any.H), 3), "\n")
cat("With 10 noise vars:", round(mean(results_noise$any.H), 3), "\n")
```

## Sensitivity Analysis: Varying Parameters

```{r sensitivity-analysis, eval=FALSE}
# Test different HR thresholds
thresholds <- c(1.10, 1.25, 1.50)
results_by_thresh <- list()

for (thresh in thresholds) {
  results_by_thresh[[as.character(thresh)]] <- foreach(
    sim = 1:100,
    .combine = rbind,
    .options.future = list(
      packages = c("forestsearch", "survival", "data.table"),
      seed = TRUE
    )
  ) %dofuture% {
    
    run_simulation_analysis(
      sim_id = sim,
      dgm = dgm_calibrated,
      n_sample = sim_config$n_sample,
      confounders_base = confounders_base,
      run_fs = TRUE,
      fs_params = modifyList(fs_params, list(hr.threshold = thresh)),
      verbose = FALSE
    )
  }
  results_by_thresh[[as.character(thresh)]]$threshold <- thresh
}

# Combine and summarize
combined <- rbindlist(results_by_thresh)
combined[, .(power = mean(any.H), ppv = mean(ppv, na.rm = TRUE)), 
         by = .(threshold, analysis)]
```

# Saving Results

```{r save-results, eval = FALSE}
# Save simulation results for later use
save_simulation_results(
  results = results_alt,
  dgm = dgm_calibrated,
  summary_table = summary_alt,
  runtime_hours = runtime_mins / 60,
  output_file = "forestsearch_simulation_alt.Rdata",
  power_approx = power_approx
)

save_simulation_results(
  results = results_null,
  dgm = dgm_null,
  summary_table = summary_null,
  runtime_hours = runtime_mins / 60,
  output_file = "forestsearch_simulation_null.Rdata"
)
```

# Complete Example Script

Here's a minimal self-contained script for running a simulation study:
 
```{r complete-example, eval=FALSE}
# ===========================================================================
# Complete ForestSearch Simulation Study - Minimal Example
# ===========================================================================

library(forestsearch)
library(data.table)
library(survival)
library(foreach)
library(doFuture)

# --- Configuration ---
N_SIMS <- 500
N_SAMPLE <- 500
TARGET_HR_HARM <- 1.5

# --- Setup parallel processing ---
plan(multisession, workers = 4)
registerDoFuture()

# --- Create DGM ---
k_inter <- calibrate_k_inter(target_hr_harm = TARGET_HR_HARM, verbose = TRUE)
dgm <- create_gbsg_dgm(model = "alt", k_inter = k_inter, verbose = TRUE)

# --- Run simulations ---
confounders <- c("z1", "z2", "z3", "z4", "z5", "size", "grade3")

results <- foreach(
  sim = 1:N_SIMS, 
  .combine = rbind,
  .options.future = list(
    packages = c("forestsearch", "survival", "data.table"),
    seed = TRUE
  )
) %dofuture% {
  run_simulation_analysis(
    sim_id = sim,
    dgm = dgm,
    n_sample = N_SAMPLE,
    max_follow = 60,
    confounders_base = confounders,
    run_fs = TRUE,
    run_fs_grf = TRUE,
    run_grf = FALSE,
    fs_params = list(hr.threshold = 1.25, fs.splits = 300, maxk = 2)
  )
}

# --- Summarize ---
summary_table <- summarize_simulation_results(results)
print(summary_table)

# --- Display formatted table ---
format_oc_results(summary_table, n_sims = N_SIMS, model_type = "alt")
```

# Summary

This vignette demonstrated the complete workflow for evaluating ForestSearch
performance through simulation:

| Step | Function | Purpose |
|------|----------|---------|
| 1. Create DGM | `create_gbsg_dgm()` | Define data generating mechanism |
| 2. Calibrate | `calibrate_k_inter()` | Achieve target subgroup HR |
| 3. Simulate | `simulate_from_gbsg_dgm()` | Generate trial data |
| 4. Analyze | `run_simulation_analysis()` | Run ForestSearch/GRF |
| 5. Summarize | `summarize_simulation_results()` | Aggregate metrics |
| 6. Display | `format_oc_results()` | Create gt tables |

**Key metrics to report:**

- **Power** (H1) / **Type I Error** (H0): Subgroup detection rate
- **Sensitivity**: P(identified | true harm subgroup)
- **Specificity**: P(not identified | true complement)
- **PPV**: P(true harm | identified)
- **NPV**: P(true complement | not identified)

# Session Info

```{r session-info, eval=TRUE}
sessionInfo()
```

# References

1. León LF, Marceau-West CT, He W, et al. (2024). "Identifying Patient Subgroups 
   with Differential Treatment Effects: A Forest Search Approach." 
   *Statistics in Medicine*.

2. Athey S, Imbens GW. (2016). "Recursive partitioning for heterogeneous causal 
   effects." *PNAS*, 113(27):7353-7360.

3. Wager S, Athey S. (2018). "Estimation and inference of heterogeneous treatment 
   effects using random forests." *JASA*, 113(523):1228-1242.
