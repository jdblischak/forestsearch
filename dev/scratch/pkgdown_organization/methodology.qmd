---
title: "ForestSearch Methodology"
subtitle: "Exploratory Subgroup Identification with Bootstrap Bias Correction"
author: "Larry F. León, Thomas Jemielita, Zifang Guo, Rachel Marceau West, Keaven M. Anderson"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    theme: flatly
    fig-width: 8
    fig-height: 6
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{ForestSearch Methodology}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(
 echo = TRUE,
 message = FALSE,
 warning = FALSE,
 fig.align = "center"
)
```

## Introduction

ForestSearch is a procedure for identifying subgroups with large treatment effects in clinical trials with survival endpoints, with particular focus on subgroups where treatment may be potentially detrimental. The approach is relatively simple and flexible, screening all possible subgroups based on hazard ratio thresholds indicative of harm with assessment according to the standard Cox model.
By reversing the role of treatment, one can also seek to identify subgroups with substantial benefit.

### Motivation

In oncology trials, subgroup analyses via forest plots are standard presentations in regulatory reviews and clinical publications. The goal is typically to evaluate the consistency of treatment effects across pre-specified subgroups relative to the intention-to-treat (ITT) population. However, there may be important subgroups based on patient characteristics that are not anticipated or well understood.

ForestSearch addresses this need for exploratory subgroup identification with the following goals:

- **Identify** an underlying subgroup $H$ consisting of subjects who derive the least benefit (or potential harm) from treatment
- **Estimate** treatment effects within identified subgroups with appropriate bias correction
- **Validate** findings through cross-validation to assess stability

### Key Features

1. **Split-sample consistency evaluation** to identify subgroups "maximally consistent with harm"
2. **Bootstrap bias correction** using infinitesimal jackknife variance estimation
3. **Variable selection** via LASSO and/or Generalized Random Forests (GRF)
4. **Cross-validation** for assessing algorithmic stability

## The ForestSearch Algorithm

The ForestSearch algorithm proceeds through four main steps:

### Step 1: Construct Candidate Factors

For candidate baseline factors $X_k$, $k = 1, \ldots, K$, construct dummy indicators for each unique factor level.

Let $l_k$ denote the unique number of values with $L = \sum_{k=1}^{K} l_k$ the number of possible single-factor subgroups.

**Example:** If $X_1$ denotes age cut at 50 years and $X_2$ denotes gender, then $L = 4$:

- age ≤ 50
- age > 50
- gender = male
- gender = female

Let $J_1, \ldots, J_L$ denote the resulting subgroup indicators. For example, for age cut at 50 years:

- $J_1 = I(\text{age} \leq 50)$ indicates membership in the "50 and younger" subgroup
- $J_2 = I(\text{age} > 50)$ indicates membership in the "older than 50" subgroup

Each $J_1, \ldots, J_L$ and non-null combinations between them (e.g., "males 50 and younger") represents a potential subgroup.

### Step 2: Enumerate Candidate Subgroups

There are $2^L - 1$ all-possible subgroup combinations. We restrict to those based on **at most two factors**. The total number of possible two-factor combinations is:

$$\binom{L}{2} + L = \frac{L(L-1)}{2} + L$$

As a minimal sample size criterion, we further restrict to candidate subgroup combinations with:

- Minimum size of 60 subjects
- Minimum of 10 events in each treatment arm

Let $\{G_s, s = 1, \ldots, S\}$ denote the collection of subgroups meeting the sample size criteria where $S \leq L(L-1)/2 + L$.

### Step 3: Screening and Consistency Evaluation

#### 3a. Hazard Ratio Screening

For subgroup $G_s$ (of size ≥ 60 and at least 20 events), estimate the Cox model log-hazard ratio $\hat{\beta}_s$, and consider the subgroup as a candidate if:

$$\hat{\beta}_s \geq \log(1.25)$$

This corresponds to a hazard ratio threshold of 1.25, indicating potential harm.

#### 3b. Split-Sample Consistency

To judge the "consistency with harm":

1. Randomly split the $G_s$ subgroup 50/50
2. Estimate the log-hazard ratio in each of these 2 random splits
3. Consider this subgroup to be "consistent with harm" if, for each random split, **both** splits have estimated log-hazard ratios ≥ log(1.0)

That is, $\min(\hat{\beta}_s^1, \hat{\beta}_s^2) \geq \log(1.0)$ for log-hazard ratio estimate pairs $\{\hat{\beta}_s^1, \hat{\beta}_s^2\}$ corresponding to each random split.

#### 3c. Consistency Rate Estimation

Repeat many times (e.g., $R = 400$) to estimate the consistency rate. Let $\{\hat{\beta}_s^{1r}, \hat{\beta}_s^{2r}\}$ denote pairs for the $r$th random split for $r = 1, \ldots, R$. The consistency rate is:

$$\hat{p}_{\text{consistency}} = \frac{1}{R} \sum_{r=1}^{R} I\left(\min(\hat{\beta}_s^{1r}, \hat{\beta}_s^{2r}) \geq 0\right)$$

### Step 4: Subgroup Selection

For subgroups with consistency rates at least 90%, choose the subgroup with the highest consistency rate as the estimated $H$, denoted $\widehat{H}$ ("maximally consistent").

If no subgroup achieves consistency ≥ 90%, then consider $H$ as null ($\widehat{H} = \emptyset$).

For the complementary group, $H^c$ is estimated as the complement of $\widehat{H}$, denoted $\widehat{H}^c$. If $\widehat{H}$ is null, then $\widehat{H}^c$ is the ITT population.

### Selection Criteria Variants

Step 4 can be modified in several ways:

| `sg_focus` | Description |
|------------|-------------|
| `"hr"` | Maximize consistency rate (default) |
| `"maxSG"` | Select largest subgroup among those with consistency ≥ threshold |
| `"minSG"` | Select smallest subgroup among those with consistency ≥ threshold |
| `"hrMaxSG"` | Among consistent candidates, select largest with best HR |
| `"hrMinSG"` | Among consistent candidates, select smallest with best HR |

Additional constraints can include median survival thresholds for the experimental arm, control arm, or both.

```{r}
#| label: algorithm-diagram
#| echo: false
#| fig-cap: "ForestSearch Algorithm Overview"

# Create a simple flow diagram
library(DiagrammeR)

grViz("
digraph ForestSearch {
 graph [rankdir = TB, fontname = Helvetica]
 node [shape = box, style = filled, fontname = Helvetica]
 
 A [label = 'Step 1: Construct\\nCandidate Factors', fillcolor = '#E8F4F8']
 B [label = 'Step 2: Enumerate\\nSubgroup Combinations', fillcolor = '#E8F4F8']
 C [label = 'Step 3a: HR Screening\\n(threshold = 1.25)', fillcolor = '#FFF3CD']
 D [label = 'Step 3b-c: Consistency\\nEvaluation (R splits)', fillcolor = '#FFF3CD']
 E [label = 'Step 4: Select\\nMaximally Consistent', fillcolor = '#D4EDDA']
 F [label = 'Bootstrap Bias\\nCorrection', fillcolor = '#F8D7DA']
 
 A -> B -> C -> D -> E -> F
}
")
```

## Asymptotic Considerations {#sec-asymptotics}

### Power Approximation

We can approximate the probability of identifying subgroup $H$ via numerical integration. Let $L_d(\beta)$ denote the Cox score statistic based on subgroup $G_s$ with a total number of $d$ observed events.

For the random splitting step, the Cox model estimates $\hat{\beta}_s^1$ and $\hat{\beta}_s^2$ are each independently approximated by $N(\beta, 8/d)$ random variables (since for both random splits $d_1 \approx d_2 \approx d/2$).

For a subgroup $H$ with underlying log-hazard ratio $\beta$, we can approximate the probability of identifying $H$ via:

$$P(\text{identify } H) \approx \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} I(w_1 + w_2 \geq 2\log(1.25)) \cdot I(w_1 \geq 0) \cdot I(w_2 \geq 0) \cdot \varphi(w_1; \beta, 8/d) \cdot \varphi(w_2; \beta, 8/d) \, dw_1 \, dw_2$$

where $\{W_1, W_2\} \sim N(\beta, 8/d)$ independently, and $\varphi(\cdot; \beta, 8/d)$ denotes the normal density with mean $\beta$ and variance $8/d$.

```{r}
#| label: power-approximation
#| fig-cap: "Approximate probability of finding H via ForestSearch"

# Power approximation function
approx_power <- function(hr, n_subgroup, cens_rate = 0.45) {
 beta <- log(hr)
 d <- (1 - cens_rate) * n_subgroup
 var_split <- 8 / d
 
 # Numerical integration
 integrate_2d <- function(beta, var) {
   # Use Monte Carlo integration
   set.seed(123)
   n_sim <- 100000
   w1 <- rnorm(n_sim, mean = beta, sd = sqrt(var))
   w2 <- rnorm(n_sim, mean = beta, sd = sqrt(var))
  
   mean((w1 + w2 >= 2 * log(1.25)) & (w1 >= 0) & (w2 >= 0))
 }
 
 integrate_2d(beta, var_split)
}

# Calculate power curves
hr_seq <- seq(0.5, 3.0, by = 0.05)
n_values <- c(60, 80, 100)

power_data <- expand.grid(hr = hr_seq, n = n_values)
power_data$power <- mapply(approx_power, power_data$hr, power_data$n)
power_data$n <- factor(power_data$n, labels = paste("n =", n_values))

library(ggplot2)
ggplot(power_data, aes(x = hr, y = power, color = n)) +
 geom_line(linewidth = 1) +
 geom_hline(yintercept = 0.10, linetype = "dashed", color = "gray50") +
 geom_hline(yintercept = 0.80, linetype = "dashed", color = "gray50") +
 geom_vline(xintercept = 1.0, linetype = "dotted", color = "gray70") +
 scale_x_continuous(breaks = seq(0.5, 3.0, by = 0.5)) +
 scale_y_continuous(breaks = seq(0, 1, by = 0.2), labels = scales::percent) +
 labs(
   x = "Underlying Hazard Ratio",
   y = "Probability of Identifying H",
   color = "Subgroup Size",
   title = "ForestSearch Power Approximation",
   subtitle = "Censoring rate ≈ 45%"
 ) +
 theme_minimal() +
 theme(legend.position = "bottom") +
 annotate("text", x = 0.6, y = 0.12, label = "10% threshold", size = 3) +
 annotate("text", x = 0.6, y = 0.82, label = "80% power", size = 3)
```

### Threshold Selection Rationale

The choice of the 1.25 and 1.0 thresholds was based on the desire to control the rate for finding a subgroup $H$ to be approximately 10% when the underlying hazard ratio for $H$ is below 1.0.

If the underlying treatment effect is uniform and beneficial, then for a random subgroup $H$, Cox model estimates will randomly fluctuate around the ITT effect.

**Example:** For $\theta^\dagger(H) \equiv \theta^\dagger(\text{ITT}) = 0.75$, the approximation yields probabilities of 0.049, 0.033, and 0.022 (for n = 60, 80, and 100, respectively), indicating reasonable control of type-1 error.

## Bootstrap Bias Correction {#sec-bootstrap}

### Sources of Bias

By the nature of the ForestSearch procedure, we expect unadjusted Cox model estimates based on $\widehat{H}$ to be **upwardly biased** due to the hazard ratio thresholds (since by construction, point estimates are ≥ 1.25 for $\widehat{H}$).

However, the bias can also be pressured in the opposite direction depending on:

- The proportion of $H^c$ subjects incorrectly included in $\widehat{H}$
- The value of $\theta^\dagger(H)$ relative to $\theta^\dagger(H^c)$

### Bias-Corrected Estimator

For bias correction, we proceed on the Cox regression coefficient scale, denoted $\hat{\beta}(\widehat{H})$, and then exponentiate to obtain point estimates and confidence intervals for hazard ratios.

Our bias-corrected estimator takes into account **two sources of bias** involving the discrepancies between the bootstrapped and observed data Cox estimators.

#### Notation

For the observed data with estimated subgroup $\widehat{H}$:

- $\hat{\beta}(\widehat{H})$: Estimated Cox model regression parameter

For bootstrap samples $b = 1, \ldots, B$ with estimated subgroup $\widehat{H}^*_b$:

- $\hat{\beta}^*_b(\widehat{H}^*_b)$: Cox model parameter for bootstrap sample based on bootstrap-estimated subgroup
- $\hat{\beta}(\widehat{H}^*_b)$: Cox model parameter for observed data based on bootstrap-estimated subgroup
- $\hat{\beta}^*_b(\widehat{H})$: Cox model parameter for bootstrap sample based on observed subgroup

#### Bias Terms

Define the bias terms:

$$\eta^*_b(\widehat{H}^*_b) = \hat{\beta}^*_b(\widehat{H}^*_b) - \hat{\beta}(\widehat{H}^*_b)$$

$$\eta^*_b(\widehat{H}) = \hat{\beta}^*_b(\widehat{H}) - \hat{\beta}(\widehat{H})$$

#### Bias-Corrected Estimators

The bias-corrected estimators are defined as:

$$\hat{\beta}^*(\widehat{H}) = \hat{\beta}(\widehat{H}) - \frac{1}{B}\sum_{b=1}^{B} \left[\eta^*_b(\widehat{H}^*_b) + \eta^*_b(\widehat{H})\right]$$

$$\hat{\theta}^*(\widehat{H}) = \exp\left(\hat{\beta}^*(\widehat{H})\right)$$

Similarly for the complement:

$$\hat{\beta}^*(\widehat{H}^c) = \hat{\beta}(\widehat{H}^c) - \frac{1}{B}\sum_{b=1}^{B} \left[\eta^*_b(\widehat{H}^{c*}_b) + \eta^*_b(\widehat{H}^c)\right]$$

### Infinitesimal Jackknife Variance Estimation

To estimate the variance, we apply an **infinitesimal jackknife approximation**, viewing the bias-corrected estimators as "bagged estimators."

Let $O^*_b = \{O^*_{b1}, O^*_{b2}, \ldots, O^*_{bN}\}$ denote bootstrap sample $b$. Let $K^*_{bi} = \#\{O^*_{bj} = O_i\}$ denote the number of times observation $O_i$ is drawn for the $b$th bootstrap sample, and let $\bar{K}^*_i = (1/B)\sum_{b=1}^{B} K^*_{bi}$.

The infinitesimal jackknife variance estimate is:

$$\tilde{V} = \sum_{i=1}^{N} \widetilde{\text{cov}}_i^2$$

where:

$$\widetilde{\text{cov}}_i = \frac{1}{B} \sum_{b=1}^{B} (K^*_{bi} - \bar{K}^*_i) \left[\hat{\beta}(\widehat{H}) - \eta^*_b(\widehat{H}^*_b) - \eta^*_b(\widehat{H}) - \hat{\beta}^*(\widehat{H})\right]$$

The bias-corrected variance is:

$$\hat{V} = \tilde{V} - \frac{N}{B} \tilde{\sigma}^2_B$$

where:

$$\tilde{\sigma}^2_B = \frac{1}{B} \sum_{b=1}^{B} \left[\hat{\beta}(\widehat{H}) - \eta^*_b(\widehat{H}^*_b) - \eta^*_b(\widehat{H}) - \hat{\beta}^*(\widehat{H})\right]^2$$

Confidence intervals for hazard ratios are based on standard normal approximations (exponentiated).

```{r}
#| label: bootstrap-diagram
#| echo: false
#| fig-cap: "Bootstrap Bias Correction Workflow"

grViz("
digraph Bootstrap {
 graph [rankdir = LR, fontname = Helvetica]
 node [shape = box, style = filled, fontname = Helvetica]
 
 subgraph cluster_0 {
   label = 'Observed Data'
   style = filled
   color = '#E8F4F8'
   A [label = 'Run ForestSearch\\nGet Ĥ', fillcolor = white]
   B [label = 'Estimate β̂(Ĥ)', fillcolor = white]
 }
 
 subgraph cluster_1 {
   label = 'Bootstrap b = 1,...,B'
   style = filled
   color = '#FFF3CD'
   C [label = 'Resample Data', fillcolor = white]
   D [label = 'Run ForestSearch\\nGet Ĥ*_b', fillcolor = white]
   E [label = 'Compute η*_b(Ĥ*_b)\\nand η*_b(Ĥ)', fillcolor = white]
 }
 
 subgraph cluster_2 {
   label = 'Bias Correction'
   style = filled
   color = '#D4EDDA'
   F [label = 'Average Bias\\nTerms', fillcolor = white]
   G [label = 'β̂*(Ĥ) = β̂(Ĥ) - bias', fillcolor = white]
   H [label = 'IJ Variance\\nEstimation', fillcolor = white]
 }
 
 A -> B
 B -> C
 C -> D -> E
 E -> F -> G
 F -> H
}
")
```

## Cross-Validation {#sec-cv}

Cross-validation is used for evaluating the quality and stability of the selection algorithm.

### N-Fold (Leave-One-Out) Cross-Validation

For N-fold CV, we exclude each subject ($i = 1, \ldots, N$) from the analysis and predict their $\widehat{H}$ (or $\widehat{H}^c$) classification based on the remaining $N-1$ subjects.

Let $\hat{\pi}^{-i}(\mathbf{Z}_i)$ denote the $i$th subject's predicted classification based on the FS procedure without the subject in the analysis.

Similarly, $\hat{\pi}(\mathbf{Z}_i)$ is the FS classification based on the full sample analysis.

**Interpretation:**

- Correspondence between $\hat{\pi}(\cdot)$ and $\hat{\pi}^{-i}(\cdot)$ subgroup analysis results may be anticipated, especially for large $N$
- Substantial lack of correspondence may suggest underlying instability

### K-Fold Cross-Validation

In K-fold CV (e.g., 10-fold):

1. Randomly partition the data into K folds
2. For each fold (leaving these subjects out), select $\widehat{H}$ based on the other K-1 folds
3. Predict the classification for the left-out fold

Since this process depends on the random partition, repeat 50-200 times and summarize correspondence measures across the partitions.

### CV Metrics

The sensitivity and positive predictive value metrics are modified by replacing $\widehat{H}$ with $\widehat{H}^{-i}$ and the true $H$ with $\widehat{H}$:

$$\text{sensCV}(\widehat{H}) = \frac{\#\{i \in \widehat{H}^{-i} \cap \widehat{H}\}}{\#\{i \in \widehat{H}\}}$$

This denotes the correspondence between the CV "testing prediction" and the full analysis $\widehat{H}$-classification.

```{r}
#| label: cv-metrics-table
#| echo: false

cv_metrics <- data.frame(
 Metric = c("sensCV(Ĥ)", "sensCV(Ĥᶜ)", "ppvCV(Ĥ)", "ppvCV(Ĥᶜ)", "Exact Match"),
 Description = c(
   "Proportion of full-analysis Ĥ subjects also classified as Ĥ in CV",
   "Proportion of full-analysis Ĥᶜ subjects also classified as Ĥᶜ in CV",
   "Proportion of CV Ĥ subjects that match full-analysis Ĥ",
   "Proportion of CV Ĥᶜ subjects that match full-analysis Ĥᶜ",
   "Proportion of CV folds reproducing exact subgroup definition"
 ),
 Interpretation = c(
   "Higher = more stable Ĥ identification",
   "Higher = more stable Ĥᶜ identification",
   "Higher = CV predictions align with full analysis",
   "Higher = CV predictions align with full analysis",
   "Higher = algorithm consistently identifies same subgroup"
 )
)

knitr::kable(cv_metrics, caption = "Cross-Validation Metrics for ForestSearch")
```

## Variable Selection Methods

### LASSO

LASSO (Least Absolute Shrinkage and Selection Operator) is used for Cox model covariate (prognostic) selection. In simulations, LASSO helps mitigate false discovery when analyses include baseline factors that are completely random noise.

```r
# LASSO is enabled by default
result <- forestsearch(
 df.analysis = data,
 use_lasso = TRUE,
 use_grf = FALSE,
 ...
)
```

### Generalized Random Forests (GRF)

GRF targets RMST (Restricted Mean Survival Time) and can be used as a complementary variable selection method. GRF is itself a subgroup identification procedure, but in ForestSearch it is used for identifying candidate factors (binary splits).

```r
# Enable both LASSO and GRF
result <- forestsearch(
 df.analysis = data,
 use_lasso = TRUE,
 use_grf = TRUE,
 ...
)
```

### Recommendations

| Scenario | Recommendation |
|----------|----------------|
| Standard analysis | `use_lasso = TRUE, use_grf = FALSE` |
| Exploratory with many candidates | `use_lasso = TRUE, use_grf = TRUE` |
| When noise factors may be present | Always include LASSO |
| Large datasets with complex interactions | Consider GRF for variable importance |

## Practical Considerations

### Sample Size Requirements

- **Minimum subgroup size:** 60 subjects (default `n.min = 60`)
- **Minimum events:** 10-12 per treatment arm (default `d0.min = 12, d1.min = 12`)
- **Recommended trial size:** N ≥ 300 for Phase 2; N ≥ 500 for Phase 3

### Computational Considerations

The computational time depends on:

- Number of candidate factors (K)
- Number of subgroup combinations meeting size criteria (S)
- Number of consistency splits (`fs.splits`, default 400-1000)
- Number of bootstrap iterations (B, typically 300-2000)
- Number of CV repetitions

**Typical timing** (Apple M1, 20 cores):

| Component | Time |
|-----------|------|
| FS analysis | ~0.2 minutes |
| 2000 bootstraps | ~30 minutes |
| N-fold CV | ~4-22 minutes |
| 200 × 10-fold CV | ~60-105 minutes |

### Interpretation Guidelines

1. **Bias-corrected estimates** should be reported alongside unadjusted estimates
2. **Cross-validation metrics** provide diagnostic value for stability
3. **Biological plausibility** should be evaluated using external knowledge
4. **Results are exploratory** and should inform future trial design rather than definitive conclusions

## References

::: {#refs}
:::

## Session Information

```{r}
#| label: session-info
sessionInfo()
```
