---
title: "Subgroup Identification Analysis"
subtitle: "Applications to consistency analyses in MRCTs"
author: "Larry Leon"
toc: true
number-sections: true
toc-depth: 3
format: 
  html:
    self-contained: true
    code-fold: true
    bibliography: references.bib
    fig_caption: yes
    keep_tex: yes
    latex_engine: xelatex
    citation_package: natbib
    fig_width: 10
    fig_height: 8
    fontsize: 10pt
    extra_dependencies: "subfig"
---


```{r Setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)

options(warn = -1)

rm(list=ls())

library(tinytex)
library(ggplot2)
library(table1)

library(gt)

# test these packages
# just for testing any conflicts
# library(tidyverse)
# library(plyr)
# library(dplyr)
# library(glmnet)

library(survival)
library(data.table)
library(randomForest)
library(grf)
library(policytree)
library(DiagrammeR)

library(grid)
library(forestploter)
library(randomizr)

# library(devtools)
# install_github("larry-leon/weightedsurv", force = TRUE)
# install_github("larry-leon/forestsearch", force = TRUE)


library(forestsearch)
library(weightedsurv)

#help(forestsearch)
#help(generate_aft_dgm_flex)


```


# Summary

We evaluate the potential for utilizing subgroup analyses for enhancing regional consistency analyses in oncology multi-regional clinical trials (MRCTs).   We consider a setting 
where asia pacific (AP) countries consist of approximately $15-20\%$ of a global trial and the criteria for consistency is a positive point estimate (Cox model hazard-ratio $< 1.0$) while 
the global trial meets statistical significance.   

Suppose the following -- 

**ITT meets statistical significance but the AP regional analysis appears problematic (hazard-ratio estimate $\approx 1.0$)**

Can a subgroup with enhanced benefit be identified in the non-AP population which then translates to the AP population?   

That is, suppose there exists a subgroup ${\cal G}$ such that the treatment affect 
is more pronounced and (standard) Cox model analyses based on the ${\cal G}$ subgroup of AP (AP$({\cal G})$, say) enables meeting consistency.

We assume Cox model analyses are the primary analysis where only the treatment arm is included as a covariate (with or without stratification by randomization factors). 

While there are a plethora of subgroup identification approaches available we consider the *forestSearch* approach, @Leon_2024, which was developed for identifying strong subgroup treatment effects (i.e., strong HTEs).  

We consider three objectives:  (A) Identify a subgroup with the largest detrimental effect (*harm*, say) defined in terms of the largest hazard-ratio estimate exceeding a clinical threshold in favor of control (e.g., $\hat\beta \geq \log(0.90)$);  (B) Identify the *smallest* subgroup with a hazard-ratio estimate indicative of *harm* (e.g., $\hat\beta \geq \log(0.90)$); and (C) Identify the *largest* subgroup with strong benefit (e.g., $\hat\beta \leq \log(0.60)$).
These objectives are related with the overarching goal of identifying a subgroup with enhanced benefit;   In (A) and (B) we identify a subgroup indicative of *harm*, $\hat{\cal H}$ say, in which case the complementary subgroup (${\hat{\cal H}}^{c}:= {\hat{\cal G}}$) may be generally beneficial.  Whereas in (C) we are more directly *targeting* a strong (clinically compelling) beneficial subgroup effect.  

In this note we provide a brief review of the Weibull model in Section 2 where we describe a (2-phase) spline model that we utilize for inducing differential treatment effects by way of a ''biomarker''.  Section 3 provides examples of how to generate ''biomarker profiles'' of interest where one can define subgroups based on biomarker cutpoints which correspond to underlying causal hazard ratio effects (e.g., causal hazard-ratio effect for biomarker $\geq 2$ is approximately $0.5$, say).  Section 4 describes the simulation setup (data generating model/process) and generates an example dataset used for analysis illustration where in Section 5 we discuss details of implementing (A)-(C) above.  In Section 6 we evaluate the operating characteristics for objective (B) under conditions where there exists a strong biomarker effect, and under the null of a uniform treatment benefit (i.e., ${\cal G} = \emptyset$).   Lastly, Section 7 briefly discusses estimation (de-biased point and CI estimates) -- taking into account the identification algorithm -- where there is interest in inference regarding the population from which the subgroup was identified (e.g., non-AP).  

Throughout this note R code is provided and illustrated based on real trial data (subject-level baseline characteristics are fixed at their observed quantities).  Our hope is that these notes can serve as a framework for future trial planning as well as for simulating from study data to understand the operating characteristics for teams and external discussions.

A diagram illustrating the general flow follows.


```{r}
#| fig-cap: "Subgroup identification analysis based on non-AP data to identify subgroups of marked benefit in AP. Operating characteristics evaluates whole process starting from analysis option"
#| label: analysis-diagram
library(png)
library(grid)
img <- readPNG('MRCT-Flow.png')
grid.raster(img)
```



# Weibull AFT/Cox model with biomarker effects

## Brief Review
For the Weibull distribution with shape parameter $\nu$ and scale parameter $\theta$ the
density, cdf, survival, hazard and cumulative hazard functions are:
\begin{eqnarray*}
f(t) &=& \nu \theta^{-\nu}t^{\nu-1}\exp(-(t/\theta)^{\nu}), \cr F(t)
&=& \int_{0}^{t}\nu \theta^{-\nu}s^{\nu-1}\exp(-(s/\theta)^{\nu})ds
\cr &=& \int_{0}^{(t/\theta)^{\nu}}e^{-w}dw \cr & &
(w=(s/\theta)^{\nu},dw=\nu s^{\nu-1}\theta^{-\nu}ds) \cr
&=&1-\exp(-(t/\theta)^{\nu}), \cr S(t) &=& \exp(-(t/\theta)^{\nu}),
\cr \lambda(t)&=&\nu \theta^{-\nu}t^{\nu-1}, \cr \Lambda(t) &=&
-\log(S(t))=(t/\theta)^{\nu}.
\end{eqnarray*}
Note that here we define the density to correspond with R's definition.
For shape parameter $\nu \in (0,1)$ the hazard is strictly decreasing in $t \geq 0$,
whereas for $\nu >1$ the hazard is strictly increasing in $t \geq 0$.  

*Note*: $\Lambda(T) \sim E(1)$

The cumulative hazard function $\Lambda(\cdot)$ evaluated at $T$, $\Lambda(T)$, as a random variable has cdf
$$\eqalign{ \Pr(\Lambda(T) \leq t) &=\Pr(-\log(1-F(T)) \leq t) =\Pr(1-F(T) \geq e^{-t}) \cr
&=\Pr(T \leq F^{-1}(1-e^{-t})) =F(F^{-1}(1-e^{-t}))  = 1-e^{-t}, \cr}$$
\noindent
which is the CDF of the exponential distribution, $E(1)$ (say). 


In the following we use 

\begin{equation}
\tag{1}
\Lambda(T) \sim E(1)
\end{equation}
\noindent
to represent the Weibull regression model as an AFT model which is also a Cox model. 

Now, $\Lambda(T)=(T/\theta)^{\nu}$ and write
$Q=-\log(S(T))=\Lambda(T)=(T/\theta)^{\nu}$, where from (1), $Q \sim E(1)$.  That is
$\log(Q)=\nu(\log(T)-\log(\theta))$ can be expressed as

\begin{equation}
\tag{2}
\log(T)=\log(\theta)+ \tau \log(Q) := \log(\theta) + \tau \epsilon,
\end{equation}
\noindent
where $\tau=1/\nu$ and it is easy to show that $\epsilon=\log(Q)$ has the ``extreme
value'' distribution with density $f_{\epsilon}(x)=\exp(x-e^{x})$ for $x \in {\cal R}$.
Here the range of $\log(T) \in {\cal R}$ is un-restricted.  The *survReg* routine uses the parameterization $(2)$ and therefore estimates
$\log(\theta)$ and $\tau=1/\nu$.

To incorporate covariates $L$ (say) we specify
$$\lambda(t;L)=\big(\nu \theta^{-\nu}t^{\nu-1} \big) \exp(L'\beta)
:= \lambda_{0}(t)\exp(L'\beta),$$
\noindent
where $\lambda_{0}(t)$ is the hazard, say, for $T_{0} \sim \hbox{Weibull}(\nu,\theta)$.
This is a special case of the proportional hazards model. The chf (conditional chf with covariate vector
$L$) is $\Lambda(t;Z)=(t/\theta)^{\nu}\exp(L'\beta)$ so that
analogous to above this leads to the representation

\begin{equation}
\tag{3}
\log(T) =\log(\theta)+\tau[-L'\beta+\epsilon] =\log(\theta)+L'\gamma + \tau \epsilon,
\end{equation}
\noindent
where $\gamma=-\tau\beta$, with $\tau$ and $\epsilon$ defined in (2).  R *survReg* uses this AFT parameterization so that the estimated components of
$\gamma$, $\gamma_{p}$ say, are that of $-\tau\beta_{p}$ for $p=1,\ldots,m$ ($m$ is dimension of $\beta$).

When fitting the AFT model (3) via suvreg we therefore transform parameters $\hat\gamma$ to the Weibull hazard-ratio parameterization (2) via

\begin{equation}
\tag{4}
\hat\beta = -\hat\gamma / \hat{\tau}.
\end{equation}


As an illustration we compare the *survReg* model fits for the case-study dataset.  The following table below compares the Weibull *survReg* model fits with covariates Treat and Ecog1 (Ecog = 1 vs 0) where components of $\hat\gamma$ from model (3) are calculated according to *survReg* and $\hat\beta$ are formed via (4).  In the table below Weibull estimates of $\hat\beta$ are compared to Cox model versions.
  
```{r Case-study}
# case-study example
# Draw sample from synthetic dataset (n=2,000)
dfsynth <- read.table("data/dfsynthetic.csv", header=TRUE, sep=",")
# Consider the "full" dataset 
# will sample trials (eg, n=500) below
df.case <- dfsynth

# Draw sample of n=500
# ndraw <- 500
# set.seed(8316951)
# id_draw <- sample(1:nrow(dfsynth), ndraw)
# df.case <- dfsynth[id_draw,]
#names(df.case)
```


```{r, fig.width = 12, fig.height = 6}
# Original data

dfc <- df_counting(
  df = df.case,
  by.risk = 6,
  tte.name = "tte", 
  event.name = "event", 
  treat.name = "treat"
)

plot_weighted_km(dfc, conf.int = TRUE, show.logrank = TRUE, ymax = 1.05)

```


```{r Weibull-vs-Cox}
# Comparing Weibull vs Cox with case-study where outcomes are artifical
# This is just for illustration to show conversion of Weibull parameters from 
# AFT regression to Weibull hazard 
fit.weib_ex <- survreg(Surv(tte,pmax(event,1)) ~ treat + ecog, dist='weibull', data=df.case)
tauhat <- fit.weib_ex$scale
# convert (treat,ecog1) regression parms to Weibull hazard-ratio
bhat.weib <- -(1)*coef(fit.weib_ex)[c(2,3)]/tauhat
# Compare to Cox 
fit.cox_ex <- coxph(Surv(tte,pmax(event,1)) ~ treat + ecog, data=df.case)
res <- cbind(bhat.weib,coef(fit.cox_ex))
res <- as.data.frame(res)
colnames(res)<-c("Weibull","Cox")
res |> gt() |>
fmt_number(columns=1:2,decimals=6) |>
tab_header(title="Comparing Weibull to Cox hazard ratio estimates",
subtitle="Case-study dataset with artificial outcomes")

```



## Biomarker effects with spline model  

We now outline how potential outcomes are simulated according to parameters fit to the case-study dataset but with parameters specified to induce biomarker effects.  That is, causal treatment effects (on log(hazard-ratio) scale) that follow 
a spline model according to patterns where biomarker effects increase with biomarker levels;  Including various degrees of limited treatment effects for low biomarker levels.

We first consider a Weibull model with treatment and a single biomarker covariate $Z$ where we write the linear predictor of the Cox model $L'\beta$ (say) as 

\begin{equation}
\tag{5}
L'\beta  := \beta_{1}\hbox{Treat} + \beta_{2}\hbox{Z} + \beta_{3}\hbox{Z}\hbox{Treat} + \beta_{4}(\hbox{Z}-k)I(\hbox{Z}>k) + \beta_{5}(\hbox{Z}-k)I(\hbox{Z}>k)\hbox{Treat}.
\end{equation}

Following the potential-outcome approach let $l_{x,z}$ denote subject's hazard-function "had they followed treatment regimen $Treat=x$ while having biomarker level $Z=z$".   That is, for subject with biomarker level $Z=z$ we can simulate 
their survival outcomes under both treatment ($x=1$) and control ($x=0$) conditions.   Let $\beta^{0} = (\beta_{1}^{0},\ldots,\beta_{5}^{0})'$ denote the true coefficients and denote the hazard function as 

$$\lambda_{x,z}(t) = \lambda_{0}(t)\exp(l_{x,z}), \quad \hbox{say}.$$  

Writing 

\begin{equation}
l_{x,z} = \beta^{0}_{1}x + \beta^{0}_{2}z + \beta^{0}_{3}zx + \beta^{0}_{4}(z-k)I(z>k) + 
 \beta^{0}_{5}(z-k)I(z>k)x,
\end{equation}
\noindent
the log of the hazard ratio for biomarker level $z$ under treatment ($x=1$) relative to control ($x=0$) is given by 

\begin{equation}
\tag{6}
\psi^{0}(z) := \log(\lambda_{1,z}(t)/\lambda_{0,z}(t)) = \beta^{0}_{1} + \beta^{0}_{3}z + \beta^{0}_{5}(z-k)I(z>k).
\end{equation}


## Causal log-hazard-ratio

The log(hazard-ratio) for biomarker level $z$ is a linear function of $z$ with a change-point (in slope) at $z=k$ given by 

\begin{eqnarray*}
\psi^{0}(z) &=& \beta^{0}_{1} + \beta^{0}_{3}z, \quad \hbox{for} \ z \leq k, \cr
            &=& \beta^{0}_{1} + \beta^{0}_{3}z + \beta^{0}_{5}(z-k), \quad \hbox{for} \ z > k.
\end{eqnarray*}

Log hazard-ratio parameters $(\beta^{0}_{1},\beta^{0}_{3},\beta^{0}_{5})$ can be chosen to generate "treatment effect patterns" by specifying $\psi^{0}(z)$ values at 
$z=0$, $z=k$, and $z=\zeta$ for $\zeta > k$.  For specified $\psi^{0}(0)$, $\psi^{0}(k)$, and $\psi^{0}(\zeta)$ we have 

\begin{eqnarray}
\beta^{0}_{1} &=& \psi^{0}(0), \cr
\beta^{0}_{3} &=& {(\psi^{0}(k) - \beta^{0}_{1}) \over k}, \cr
\beta^{0}_{5} &=& {(\psi^{0}(\zeta) - \beta^{0}_{1} - \beta^{0}_{3}\zeta) \over (\zeta -k)}.
\end{eqnarray}


The function *get_dgm_stratified* generates $\psi^{0}(z)$ according to desired "biomarker treatment effect patterns" as follows.

- Let $X$ and $Z$ denote the treatment and biomarker variables in the case-study dataset and for specified $k$, form the covariates $L:=(X,Z,ZX,(Z-k)I(Z>k),(Z-k)I(Z>k)X))$;
- Fit the Weibull model (recall on AFT scale) to get $\log(\hat\theta)$, $\hat\tau=1/\hat{\nu}$, and $\hat\gamma$ corresponding to $L$;
- $\hat\gamma$ is in terms of the AFT parameterization given by model (3)
- Next transform to the Weibull (Cox) log hazard-ratio parameterization (4): $\hat\beta = -\hat\gamma/\hat\tau$
- Set "true" parameters $\theta^{0}=\hat\theta$, and $\tau^{0}=\hat\tau$
- Initialize the "true" parameter $\beta^{0} = \hat\beta$ and re-define parameters 1, 3, and 5 in order to satisfy specified $\psi^{0}(0)$, $\psi^{0}(k)$, and $\psi^{0}(\zeta)$:
$\beta^{0}[1] = \psi^{0}(0)$, $\beta^{0}[3] = (\psi^{0}(k) - \beta^{0}[1])/k$, and $\beta^{0}[5] = (\psi^{0}(\zeta) - \beta^{0}[1] - \beta^{0}[3]\zeta)/(\zeta -k)$;
- Form corresponding $\gamma^{0}= -\beta^{0}\tau^{0}$
- For simulations we use the AFT parameterization (3) to generate $\log(T)$ outcomes according to $\log(T) = \log(\theta^{0}) + L'\gamma^{0} + \tau^{0}\epsilon$ where recall $\epsilon$ has the ``extreme value'' distribution.


The inputs of the *get_dgm_stratified* function are: 

- The case-study dataset ("df")
- "knot"=$k$, "zeta"=$\zeta$, and "log.hrs"= ($\psi^{0}(0),\psi^{0}(k),\psi^{0}(\zeta))$
- Note that *get_dgm_stratified* allows for outcomes to follow a stratified Weibull (Cox) models in which case the log(hazard-ratio) effects will depend on the strata, "strata_tte", where the default is non-stratified ("strata_tte=NULL")
- If stratified the $\tau$ parameters and hence $\gamma^{0}$ depend on the strata
- If stratified, then to simplify, the $\gamma^{0}$ effects are calculated based on the median of the $\tau^{0}$'s ($\tau^{0}=\tau_{med}$, say).  



To-do --> explain outputs and how used in *draw_sim_stratified* ...


## Example where treatment effects increase with increasing biomarker 

We assume that $z=0$ indicates biomarker negative values with $z>1$ indicating positive levels.  As an example, suppose the log(hazard ratio) at
$z=0$ is $\psi^{0}(0)=\log(3)$ and decreases linearly for $z \leq 5$ (with slope $\beta_{3}^{0}$) such that at $z=5$, $\psi^{0}(5)=\log(1.25)$ and 
for $z>5$ decreases linearly (with a change in slope and intercept) such that at $z=10$, $\psi^{0}(10) = \log(0.5)$.  In the following we will describe summary measures of the treatment effects as a function of increasing [or decreasing] values of the biomarker.  In this example treatment is detrimental 
for lower values of the biomarker with treatment effects increasing fairly quickly as the biomarker increases with an overall ``average hazard ratio'' (AHR) of $\approx 0.74$ (see Working Example below).  


## Including prognostic factors $W$

We extend the model to include a baseline prognostic factor $W$ within $L$ (effect parameter $\beta^{0}_{w}$) where

\begin{equation}
l_{x,z,w} = \beta^{0}_{1}x + \beta^{0}_{2}z + \beta^{0}_{3}zx + \beta^{0}_{4}(z-k)I(z>k) + 
 \beta^{0}_{5}(z-k)I(z>k)x + \beta^{0}_{w}w.
\end{equation}


Note that defining $m(1,z,w)$ [$m(0,z,w)$)] as the conditional expected value of $\log(T)$ with treatment set to experimental [control] given biomarker level $Z=z$ and $W=w$ 

$$\psi^{0}(z,w) = \{m(0,z,w)-m(1,z,w)\}/\tau$$
\noindent
which corresponds to the difference in the (true) means of the potential outcomes under experimental and control conditions ($\log(T[0,z,w]$ versus $\log(T[1,z,w]$, say) 


# Specifying biomarker treatment effects

## Average hazard ratios (AHRs)

We define the *biomarker average hazard ratio (AHR)* as the expected value of $\psi^{0}(\cdot)$ across "increasing biomarker" sub-populations.  For example, $\hbox{AHR}(2^{+})$ represents the AHR for subjects with biomarker values $\geq 2$ via  


\begin{equation}
\tag{6}
\hbox{AHR}(z^{+}):= \exp\left\{E_{Z \geq z} \psi^{0}(Z) \right\};
\end{equation}

With the opposite direction -- across "decreasing biomarker" sub-populations

\begin{equation}
\tag{7}
\hbox{AHR}(z^{-}):= \exp\left\{E_{Z \leq z} \psi^{0}(Z) \right\}.
\end{equation}

Here the expectations are over the distribution of $Z$, however if there is a prognostic factor $W$ then the expectations are over $(Z,W)$.  In our calculations we calculate empirical averages.


## Controlled direct-effect (CDE) versions (@ACR_2015)

Averaging across hazard ratios:  Define the hazard (omitting baseline hazard) $\theta^{x}(z,w) = \exp(l(x,z,w))$ setting treatment to $X=x$ given $Z=z$ and $W=w$.  Aalen et al. (@ACR_2015) define the controlled direct-effect (CDE) as the ratio of the expected hazards.  Here we consider the above cumulative versions (omitting possible dependence on $W$).   Let $\bar\theta^{x}(z+) = E_{Z \geq z}\theta^{x}(Z)$ for $x=0,1$, and define

$$\hbox{CDE}(z^{+}):= \bar\theta^{1}(z+)/\bar\theta^{0}(z+), \quad \hbox{and} \quad \hbox{CDE}(z^{-}):= \bar\theta^{1}(z-)/\bar\theta^{0}(z-)$$


## Compare 'AHR' versions under various biomarker specifications 

### Under PH (uniform effects, hr=0.7), with strong prognostic effect $\beta_{w}= -\log(5)$


```{r}
# Outcome model is NON-stratified (strata_tte=NULL)
# PH 

# Create AP region flag
df.case$AP <- df.case$region_asia

# Spline biomarker effects (based on "bm" spline)

dgm_spline <- generate_aft_dgm_flex(
  data = df.case,
  continuous_vars = c("age", "bm"),
  factor_vars = c("male", "histology", "prior_treat","AP"),
  set_beta_spec = list(set_var = c("z_AP"), beta_var = -log(5)),
  outcome_var = "tte",
  event_var = "event",
  treatment_var = "treat",
  cens_type = "uniform",
  subgroup_vars = NULL,
  subgroup_cuts = NULL,
  model = "alt",
  spline_spec = list(
    var = "z_bm",  # Use original variable name (or "z_er" if transformed)
    knot = 5,
    zeta = 10,
    log_hrs = log(c(2, 1.25, 0.5))  # HR varies with ER
  ),
  k_inter = 0.0,  # Amplify or turn-off (=0) subgroup per subgroup_vars effect?
  verbose = TRUE
)


# dgm_spline <- generate_aft_dgm_flex(
#   data = df.case,
#   continuous_vars = c("bm"),
#   factor_vars = c("AP"),
#   set_beta_spec = list(set_var = c("z_AP"), beta_var = -log(4)),
#   outcome_var = "tte",
#   event_var = "event",
#   treatment_var = "treat",
#   subgroup_vars = NULL, 
#   subgroup_cuts = NULL,
#   model = "alt",
#   spline_spec = list(
#     var = "z_bm",  # Use original variable name (or "z_er" if transformed)
#     knot = 2,
#     zeta = 5,
#     log_hrs = log(c(2.5, 1.0, 0.5))  # HR varies with ER
#   ),
#   k_inter = 0.0,  # Amplify or turn-off (=0) subgroup per subgroup_vars effect?
#   verbose = TRUE
# )


# Examine beta = log(hr) parameters

dgm_spline$model_params$b0
cat("AP region parmeter = -log(5)?", c(dgm_spline$model_params$b0["z_AP"],-log(5)),"\n")


```


```{r,  fig.width = 8, fig.height = 5}
# Plot the spline effect
plot_spline_treatment_effect(dgm_spline)
```



```{r,  fig.width=12, fig.height=10}

 results <- cox_ahr_cde_analysis(
   df = dgm_spline$df_super,
   tte_name = "y", event_name = "event", treat_name = "treat", 
   z_name = "z_bm",
   hr_threshold = 0.9,
   plot_style = "grid"
 )

 # Access specific results
 print(results$subgroup_stats)
 print(results$optimal_cutpoint)

```


```{r, fig.width=12, fig.height=10}

# Simulate data

df_example <- simulate_from_dgm(
  dgm = dgm_spline, 
  n = 500, 
  rand_ratio = 1, 
  draw_treatment = TRUE, 
  max_follow = 100, 
  seed = 1235
)

# ITT
dfc <- df_counting(
  df = df_example,
  by.risk = 6,
  tte.name = "y_sim", 
  event.name = "event_sim", 
  treat.name = "treat_sim"
)

# No censoring
# Except by max_follow

df_example$t_true <- pmin(df_example$t_true, 100)

df_example$event_mod <- ifelse(df_example$t_true <= 100,1,0)

dfideal <- df_counting(
  df = df_example,
  by.risk = 6,
  tte.name = "t_true", 
  event.name = "event_mod", 
  treat.name = "treat_sim"
)


df_nonAP <- subset(df_example, z_AP == 0)
df_AP <- subset(df_example, z_AP == 1)


# non-AP
dfc0 <- df_counting(
  df = df_nonAP,
  by.risk = 6,
  tte.name = "y_sim", 
  event.name = "event_sim", 
  treat.name = "treat_sim"
)

# AP
dfc1 <- df_counting(
  df = df_AP,
  by.risk = 6,
  tte.name = "y_sim", 
  event.name = "event_sim", 
  treat.name = "treat_sim"
)


par(mfrow=c(2,2))

plot_weighted_km(dfideal, conf.int = FALSE, show.logrank = TRUE, ymax = 1.05)
title("ITT no random censoring")

plot_weighted_km(dfc, conf.int = FALSE, show.logrank = TRUE, ymax = 1.05)
title("ITT censored")

plot_weighted_km(dfc0, conf.int = FALSE, show.logrank = TRUE, ymax = 1.05)
title("non-AP")

plot_weighted_km(dfc1, conf.int = FALSE, show.logrank = TRUE, ymax = 1.05)
title("AP")


```


```{r, fig.width = 8, fig.height = 5}

result <- cox_cs_fit(
  df = df_example, tte_name = "y_sim", event_name ="event_sim", treat_name = "treat_sim",
  truebeta_name = "loghr_po",
  z_name = "z_bm", z_by = 1, z_window = 0.5,
  alpha = 0.20,
  plot_params = list(
    xlab = "Biomarker Level",
    main_title = "Treatment Effect Heterogeneity"
  )
)


```





Next, a simulated example with same sample size as the case-study   

-  Here we simulated a study according to the biomarker effects described above
-  Summarize the non-AP and AP populations  
-  Apply subgroup identification procedures to the non-AP data and apply to AP  


# Subgroup identification criteria:  Options for targeting subgroup effects

## Finding Non-AP subgroup with highest consistency rate (in favor of control)  

In the following we evaluate subgroups based on single-factors (e.g., "biomarker < J" say, or "Age <= 65", etc) and 
select the subgroup with the highest consistency rate with Cox hazard-ratio estimate meeting selection criteria;  The selected subgroup with the highest 
consistency rate meeting hazard ratio estimate criterion (log hazard-ratio estimate $\log(\hat\beta) \geq \log(0.90)$, say) where the "consistency rate" is at least $90$\%.

Here we are seeking to identify subgroups where the hazard ratio estimate is at least $\log(0.90)$ which may indicate limited benefit.  Further, subgroup candidates are ranked according 
to highest hazard ratio estimate and highest consistency rate.


```{r fs1, echo=TRUE, message=FALSE, warning=TRUE, fig.width = 10, fig.height = 8, eval = TRUE}

confounders.name <- c("z_age","z_bm","z_male","ecog","z_histology","z_prior_treat","strat")

dfa <- as.data.frame(df_nonAP)

# Setup parallel processing
library(doFuture)
library(doRNG)

registerDoFuture()
registerDoRNG()


system.time({fs <- forestsearch(dfa,  confounders.name=confounders.name,
                                outcome.name = "y_sim", treat.name = "treat_sim", event.name = "event_sim", id.name = "id",
                                potentialOutcome.name = "loghr_po", 
                                df.test = as.data.frame(df_AP),
                                flag_harm.name = NULL,
                                hr.threshold = 0.9, hr.consistency = 0.80, pconsistency.threshold = 0.80,
                                sg_focus = "minSG", max_subgroups_search = 30,
                                showten_subgroups = TRUE, details=TRUE,
                                conf_force = c("z_age <= 65", "z_bm <= 0", "z_bm <= 1", "z_bm <= 2","z_bm <= 5"),
                                cut_type = "default", use_grf = TRUE, plot.grf = TRUE, use_lasso = TRUE,
                                maxk = 1, n.min = 60, d0.min = 12, d1.min = 12,
                                plot.sg = TRUE, by.risk = 6,
                                parallel_args = list(plan="callr", workers = 100, show_message = TRUE)
)
})

plan("sequential")



res_tabs <- sg_tables(fs, ndecimals = 3)

res_tabs$sg10_out

res_tabs$tab_estimates


# Testing (AP)
res_tabs <- sg_tables(fs, ndecimals = 3, which_df = "testing", est_caption = "Testing (AP) dataset")
res_tabs$tab_estimates

```



```{r fs2, echo=TRUE, message=FALSE, warning=TRUE, fig.width = 10, fig.height = 8, eval = TRUE}

confounders.name <- c("z_age","z_bm","z_male","ecog","z_histology","z_prior_treat","strat")

dfa <- as.data.frame(df_nonAP)

# Setup parallel processing
library(doFuture)
library(doRNG)

registerDoFuture()
registerDoRNG()


system.time({fs <- forestsearch(dfa,  confounders.name=confounders.name,
                                outcome.name = "y_sim", treat.name = "treat_sim", event.name = "event_sim", id.name = "id",
                                potentialOutcome.name = "loghr_po", 
                                df.test = as.data.frame(df_AP),
                                flag_harm.name = NULL,
                                hr.threshold = 1.25, hr.consistency = 0.80, pconsistency.threshold = 0.80,
                                sg_focus = "minSG", max_subgroups_search = 30,
                                showten_subgroups = TRUE, details=TRUE,
                                conf_force = c("z_age <= 65", "z_bm <= 0", "z_bm <= 1", "z_bm <= 2","z_bm <= 5"),
                                cut_type = "default", use_grf = TRUE, plot.grf = TRUE, use_lasso = TRUE,
                                maxk = 1, n.min = 60, d0.min = 12, d1.min = 12,
                                plot.sg = TRUE, by.risk = 6,
                                parallel_args = list(plan="callr", workers = 100, show_message = TRUE)
)
})

plan("sequential")



res_tabs <- sg_tables(fs, ndecimals = 3)

res_tabs$sg10_out

res_tabs$tab_estimates


# Testing (AP)
res_tabs <- sg_tables(fs, ndecimals = 3, which_df = "testing", est_caption = "Testing (AP) dataset")
res_tabs$tab_estimates


```

